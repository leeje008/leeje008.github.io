[ { "title": "밑바닥부터 시작하는 딥러닝 Chapter08", "url": "/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter08/", "categories": "ML, book_study", "tags": "Deep_learning", "date": "2022-01-02 00:00:00 +0900", "snippet": "Chapter 8 딥러닝8.1 더 깊게8.1.1 더 깊은 신경망으로손글씨 숫자를 인식하는 심층 CNN(VGG 참고) 3X3 필터 층이 깊어질수록 채널 수 증가(16, 16, 32, 32, 64, 64) 풀링 계층으로 중간 데이터의 공간 크기가 점차 감소 활성화 함수는 ReLU 완전연결 계층 뒤에 드롭아웃 Adam optimizer He 초깃값학습 결과 정확도 매우 높음 인식하지 못한 이미지는 대부분 인간도 판단하기 어려운 이미지8.1.2 정확도를 더 높이려면&lt;What is the class of this image?&gt;: 다양한 데이터셋을 대상으로 다양한 기법들의 정확도 순위 정리MNIST 데이터셋 상위는 대부분 CNN 기반 그다지 깊지 않은 네트워크 MNIST 문제는 비교적 단순하기 때문정확도를 높일 수 있는 기술의 예 앙상블 학습률 감소 데이터 확장 등등데이터 확장(data augmentation) 손쉽고 확실한 방법 훈련 이미지를 인위적으로 확장하는 방법 회전 이동 crop: 이미지 일부를 잘라내는 방법 flip: 좌우 반전(대칭성을 고려하지 않아도 되는 경우에만 사용) 외형 변화: 밝기 조절 등 스케일 변화: 확대, 축소 데이터가 부족할 때 효과적8.1.3 깊게 하는 이유1. 매개변수의 수 감소5X5 합성곱 연산 vs 3X3 합성곱 2회 반복 5X5 합성곱 연산은 3X3 합성곱 연산 2회로 대체 가능 매개 변수의 수 5X5 합성곱: 25개(5X5) 3X3 합성곱 2회: 18개(2X3X3) 매개 변수 차이는 층이 깊어질수록 더욱 커진다.더욱 자세한 설명 수용 영역(receptive field): 뉴런에 변화를 일으키는 국소적인 공간 영역 작은 필터를 겹쳐 매개 변수 수를 줄여 넓은 수용 영역 소화 가능 층을 거듭할 수록 비선형인 활성화 함수를 통해 표현력 증가2. 학습의 효율성 층을 거듭할 수록 학습 데이터의 양이 줄어 고속으로 학습 가능 층을 거듭할 수록 점차 복잡한 것에 반응하는 것을 통해 알 수 있다. 층을 깊게 하면 학습해야 할 문제를 계층적을 분해할 수 있다. 즉, 각 층에서는 더욱 단순화된 문제를 풀게 된다.3. 정보를 계층적으로 전달할 수 있다.초반에는 단순한 정보를 학습하고, 그 정보를 다음 층으로 넘겨 점차 고차원적인 패턴을 학습할 수 있다.8.2 딥러닝의 초기 역사ILSVRC 2012년 AlexNet을 통해 딥러닝 주목8.2.1 이미지넷(ImageNet)데이터셋 설명 100만 장이 넘는 이미지 각 이미지에 레이블이 붙어 있다. ILSVRC에서 사용되는 데이터ILSVRC 분류(classification) 부문 1000개의 클래스 분류 채점 방식은 톱-5 오류(top-5 error): 예측 후보 클래스 5개 안에 정답이 포함되지 않을 비율 최우수 팀 성적 추이 2012년 이후 딥러닝이 선두 AlexNet이 오류 크게 개선 ResNet 150층이 넘는 신경망 사용 일반적인 인간의 인식 능력 넘어섰다고 인정(오류율 3.5%) 8.2.2 VGG 합성곱 계층과 풀링 계층으로 구성된 기본적인 CNN VGG16(16층), VGG19(19층)으로 깊은 신경망으로 심화 3X3 필터 2~4회 연속 이후 풀링 계층으로 크기를 절반으로 줄이는 과정 반복 마지막 층은 완전연결 계층 구성이 간단해서 응용하기 좋음8.2.3 GoogLeNet 인셉션 구조 사용 크기가 다른 필터와 풀링 여러 개 적용해서 결합 1X1 합성곱 연산: 채널 쪽의 크기를 줄이는 역할, 매개변수 제거와 고속 처리에 기여8.2.4 ResNet 층을 너무 깊게 했을 때 성능이 오히려 떨어지는 문제 해결 스킵 연결(skip connection) 입력 x를 연속한 두 합성곱 계층 건너뛰어 출력에 바로 연결 역전파 때 신호 감쇠를 막아준다 스킵 연결은 입력 데이터를 그대로 흐르게 해서 역전파 때 상류의 기울기를 그대로 하류로 보낸다. 기울기 소실 문제를 줄여준다. ResNet 전체 구조: VGG + skip connection전이 학습(transfer learning) 이미 학습된 가중치를 다른 신경망에 복사한 다음 그 상태로 새로운 데이터셋을 대상으로 재학습(fine tuning)을 수행해서 사용하는 것 데이터셋이 적을 때 유용 ex) 이미지넷 데이터셋으로 학습한 가중치 사용8.3 더 빠르게(딥러닝 고속화)GPU 사용8.3.1 풀어야 할 숙제AlexNet forward 처리 각 층 시간 비율: GPU(좌), CPU(우) 합성곱 계층에서 대부분 소요 단일 곱셈-누산 고속화 처리 중요8.3.2 GPU를 활용한 고속화GPU 컴퓨팅의 목적 병렬 수치 연산 고속 처리 CPU는 연속적인 복잡한 계속 처리 용이딥러닝 CPU와 GPU 학습 비교 GPU를 사용했을 때가 훨씬 빠르다 딥러닝 최적화 라이브러리 사용하면 더욱 빨라진다. 엔비디아의 CUDA8.3.3 분산 학습 다수의 GPU와 기기로 계산 분산 CNTK(computational network toolkit): 분산 학습 지원 라이브러리 거대한 데이터센터의 저지연, 고처리량 네트워크에서 학습 시 성능 크게 향상어려움 컴퓨터 사이의 통신과 데이터 동기화8.3.4 연산 정밀도와 비트 줄이기 메모리 용량과 버스 대역폭 등이 병목이 될 수 있다. 메모리 문제: 대량의 가중치 매개변수와 중간 데이터를 메모리에 저장해야 함 버스 대역폭 문제: 버스를 흐르는 데이터가 많아져 한계를 넘어서면 병목. 데이터의 비트 수 최소화 하는 것이 바람직 비트 수는 계산 정확도 vs 계산 비용, 메모리 사용량, 버스 대역폭 사이의 trade-off 신경망의 견고성: 입력 데이터가 조금 달라져도 출력 데이터는 잘 달라지지 않는 강건함을 보이는 성질 16비트 반정밀도만 사용해도 문제가 없다.(높은 수치 정밀도 요구 x) 파스칼 아키텍쳐에서 지원 파이썬: 64비트, 넘파이: 16비트도 지원 Binarized Neural Networks: 가중치와 중간 데이터를 1비트로 표현하는 방법 연구 컴퓨터의 실수 표현 방식 32비트 단정밀도 64비트 배정밀도 16비트 반정밀도8.4 딥러닝의 활용8.4.1 사물 검출 이미지 속에 담긴 사물의 위치와 종류를 알아내는 기술 하나의 이미지에 여러 사물 존재할 수 있다. R-CNN(regions with convolutional neural networks) 후보 영역 추출과 CNN 특징 계산이 가장 큰 특징 후보 영역 추출: Selective Search 기법 사용 후보 영역 추출까지 CNN으로 처리하는 Faster R-CNN 등장 8.4.2 분할(segmentation) 픽셀 수준으로 분류 픽셀 단위로 객체마다 채색된 지도 데이터를 사용해 학습 추론: 입력 이미지의 모든 픽셀 분류 모든 픽셀 대상으로 추론하면 픽셀 수만큼 forward 처리해야 되서 비효율적FCN(fully convolutional network) 한번의 forward 처리로 모든 픽셀의 클래스를 분류해주는 기법 완전연결 계층을 같은 기능을 하는 합성곱 계층으로 바꾼다. 공간 볼륨을 유지한 채 마지막 출력 처리 가능 FCN의 마지막 층: 공간의 크기 확대 이중 선형 보간(bilinear interpolation)에 의한 선형 확대 역합성곱(deconvolution) 연산으로 구현 8.4.3 사진 캡션 생성 사진을 주면 그 사진을 설명하는 글을 자동으로 생성 NIC(neural image caption) 모델 심층 CNN + 순환 신경망(recurrent neural network, RNN) CNN으로 사진 특징 추출 특징을 RNN에 넘겨 특징을 추깃값으로 텍스트를 순환적으로 생성 멀티모달 처리(multimodal processing): 여러 종류의 정보르 조합, 처리RNN 간단 설명 순환적 네트워크 구조 과거의 정보를 기억하는 특징 연속성 있는 데이터 다룰 때 주로 사용8.5 딥러닝의 미래8.5.1 이미지 스타일(화풍) 변환 두 이미지를 입력해서 새로운 그림 생성 콘텐츠 이미지 스타일 이미지 A Neural Algorithm of Artistic Style 논문학습 방식 네트워크의 중간 데이터가 콘텐츠 이미지의 중간 데이터와 비슷해지도록 학습 스타일 행렬의 오차를 줄이도록 학습해서 스타일 이미지의 화풍을 흡수하도록 한다.8.5.2 이미지 생성 대량의 이미지를 학습한 후 입력 이미지 없이 새로운 이미지 생성 DCGAN(deep convolutional generative adversarial network) 생성자(generator): 진짜와 똑같은 이미지를 생성 식별자(discriminator): 생성자가 생성한 이미지를 판별 생성자와 식별자를 겨루도록 학습해서 정교한 가짜 이미지를 생성해내도록 한다.지도학습(supervised learning)과 자율학습(unsupervised learning) 지도학습: 입력 데이터와 정답 레이블을 짝지은 데이터셋을 이용해서 학습 자율학습: 지도용 데이터 없이 스스로 학습 Deep Belief Network, Deep Boltzmann Machine 8.5.3 자율 주행 안전한 주행 영역을 인식하는 것이 중요 SegNet: 주변 환경 인식하는 CNN 기반 신경망 픽셀 수준에서 분할 8.5.4 Deep Q-Network(강화 학습)강화학습 에이전트: 주어진 환경에서 행동을 선택하는 주체 환경: 에이전트의 행동에 영향을 주는 조건 보상: 에이전트가 환경을 변화시키는 행동을 했을 때 에이전트에게 보상이 주어지고, 더 나은 보상을 받는 쪽으로 에이전트를 학습시킨다.Deep Q-Network(DQN) 딥러닝을 사용한 강화학습 Q학습에 기초 최적 행동 가치 함수로 최적 행동 정한다. 최적 가치 함수를 CNN으로 비슷하게 흉내내어 사용 입력 데이터로 게임 영상만 주면 된다. 기존의 학습에선 게임의 상태를 미리 출력해야 했다. 구성을 변경하지 않고 적용 가능 게임 영상 프레임을 입력해서 게임을 제어하는 움직임에 대햐여 각 동작의 가치를 출력 알파고, 팩맨, 아타리 등 많은 게임에서 사람보다 뛰어난 성적" }, { "title": "밑바닥부터 시작하는 딥러닝 Chapter07", "url": "/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter07/", "categories": "ML, book_study", "tags": "Deep_learning", "date": "2022-01-02 00:00:00 +0900", "snippet": "Chapter 7 합성곱 신경망(CNN)7.1 전체 구조CNN에서 등장하는 새로운 layer 합성곱 계층(convolutional layer) 풀링 계층(pooling layer)현재까지 봤던 신경망을 완전연결(fully-connected)라고 하며 Affine 계층으로 구현했다. Affine 계층 뒤에는 활성화 함수로 ReLU 혹은 Sigmoid를 사용했다.CNN의 구조는 새로운 합성곱 계층(Conv)과 풀링 계층(pooling)이 추가된다. Conv-ReLU-(Pooling)의 흐름이다. 출력층과 가까운 층의 경우에는 Affine-ReLU, 출력층은 Affine-Softmax 조합을 그대로 사용한다.▲ 완전연결 계층(위), CNN(아래)7.2 합성곱 계층 패딩(padding) 스트라이드(stride) 입체적인 데이터 흐름7.2.1 완전연결 계층의 문제점데이터의 형상이 무시된다. 완전연결 계층의 입력은 평탄화 필요 본래 다차원인 데이터의 경우 다차원에서 특별한 정보가 담겨있을 가능성이 있다. 이미지의 경우 가로, 세로, 색상의 3차원 데이터 공간적 정보 반면, 합성곱 계층은 형상을 유지한다. 입력 데이터가 형상 그대로 들어온다. 다음 계층으로 전달될 때도 그대로 전달 다차원의 형상을 가진 데이터를 올바르게 이해할 수 있다. 특징 맵(feature map): 합성곱 계층의 입출력 데이터 입력 특징 맵(input feature map) 출력 특징 맵(output feature map) 7.2.2 합성곱 연산합성곱 연산 = 필터 연산합성곱 연산 예제데이터 설명 입력 데이터: (4,4)의 높이와 너비를 가진 형상 필터: (3,3)의 높이와 너비를 가진 형상 커널이라고도 한다. 출력: (2,2)의 놆이와 너비를 가진 형상연산 과정 필터의 윈도우(window)를 일정 간격으로 이동하면서 입력 데이터에 적용 단일 곱셈-누산(fused multiply-add, FMA): 대응하는 원소기리 곱한 후 모두 더함 결과를 출력의 해당 장소에 저장 모든 장소에서 수행가중치와 편향 가중치: 필터의 매개변수 편향: 필터를 적용한 후 데이터에 더해진다. 항상 하나(1X1)만 존재7.2.3 패딩패딩(padding) 입력 데이터 주변을 특정 값으로 채우는 것 예를 들어 0으로패딩의 예 입력데이터: 4X4 패딩 후: 6X6 3X3 필터 적용 후: 4X4패딩을 하는 이유 출력 크기를 조정하는 목적 합성곱 신경망에서 그냥 필터를 적용하면 계속해서 크기가 줄어든다. 신경망이 깊어지면 어느 순간 크기가 1이 되어버린다. 이는 더이상 합성곱을 할 수 없는 상태이기 때문에 문제가 된다. 풀링을 하면 출력 크기를 유지시켜 줄 수 있어서, 입력 데이터의 공간적 크기를 고정해서 다음 층으로 넘겨줄 수 있다.7.2.4 스트라이드스트라이드 필터를 적용하는 위치 간격 스트라이드를 키우면 출력 크기가 작아진다.▲ 스트라이드가 2인 합성곱 신경망패딩, 스트라이드, 출력 크기 계산 입력 크기: $(H, W)$ 필터 크기: $(FH, FW)$ 출력 크기: $(OH, OW)$ 패딩: $P$ 스트라이드: $S$\\[OH = \\frac {H + 2P - FH} S + 1\\]\\[OW = \\frac {W + 2P - FW} S + 1\\]주의 계산 결과가 정수로 나누어 떨어져야 한다.7.2.5 3차원 데이터의 합성곱 연산3차원 데이터의 합성곱 연산 예 입력 데이터의 채널 수 = 필터의 채널 수 필터의 크기는 원하는 크기로(모든 채널의 필터 크기는 같아야 함)7.2.6 블록으로 생각하기 데이터와 필터의 형상: (채널, 높이, 너비) 출력: 채널이 1개인 특징 맵 필터를 여러 개 사용하면 출력의 채널 수도 늘어남 이 출력을 다음 층으로 넘겨준다. 필터의 가중치 데이터는 4차원: (출력 채널 수, 입력 채널 수, 높이, 너비) 편향: 채널당 하나의 값7.2.7 배치 처리 4차원으로 데이터 저장: (데이터 수, 채널 수, 높이, 너비) 가장 앞쪽에 배치용 차원을 추가 각 흐름마다 N번의 합성곱 연산을 수행 배치 처리의 효과는 완전연결 신경망과 동일7.3 풀링 계층풀링 계층 가로, 세로 방향의 공간을 줄이는 연산풀링의 예 최대 풀링(max pooling) 대상 영역 내에서 최대값을 구하는 연산 평균 풀링(average pooling) 대상 영역의 평균을 계산 이미지 인식 분야에서는 최대 풀링을 사용 윈도우 크기와 스트라이드는 동일한 값으로 하는 것이 일반적 위의 예: 윈도우 2X2, 스트라이드 2 7.3.1 풀링 계층의 특징학습해야 할 매개변수가 없다.채널 수가 변하지 않는다. 채널마다 독립적으로 계산하기 때문입력의 변화에 영향을 적게 받는다(강건하다). 입력 데이터가 조금 변해도 풀링 결과는 잘 변하지 않는다.7.4 합성곱/풀링 계층 구현하기7.4.1 4차원 배열CNN에서 흐르는 데이터는 4차원이다.import numpy as npx = np.random.rand(10, 1, 28, 28)x.shape(10, 1, 28, 28)# 첫번째 데이터에 접근x[0].shape(1, 28, 28)# 두번째 데이터에 접근x[1].shape(1, 28, 28)# 첫번째 데이터의 첫 채널에 접근x[0,0]array([[9.44322204e-01, 1.61725959e-01, 4.97299575e-01, 4.90849763e-01, 2.69806956e-01, 4.80739959e-01, 5.33766630e-01, 3.52483845e-01, 5.90821168e-01, 5.07826272e-01, 6.40826973e-01, 4.59954326e-01, 5.82373841e-01, 6.52799925e-02, 3.35695899e-01, 4.61650460e-01, 8.95937799e-01, 6.28431996e-02, 6.36711673e-01, 5.50039978e-01, 8.44648519e-01, 4.38516371e-01, 5.49044826e-01, 8.53925508e-01, 1.25532691e-01, 9.38128354e-01, 3.10436399e-01, 6.31229430e-02], ....... , [1.09660007e-01, 3.29975706e-01, 6.27062235e-01, 2.80376880e-02, 3.70181133e-01, 2.82339621e-01, 2.81632554e-01, 1.34358950e-01, 5.66760294e-01, 2.94240600e-01, 5.15104998e-01, 1.93230444e-02, 5.47469657e-01, 2.10305031e-01, 3.66689889e-02, 5.87465571e-01, 3.07306557e-01, 8.89560760e-01, 5.15682357e-01, 8.76126342e-01, 7.75768722e-01, 9.25446546e-01, 3.85371285e-01, 7.81498205e-01, 1.90269789e-01, 9.38474719e-01, 1.95191469e-01, 8.50734947e-01]])7.4.2 im2col로 데이터 전개하기넘파이에서 원소를 접근할 때 for 문을 사용하지 않는 것이 바람직하다.im2col 함수 입력 데이터를 필터링(가중치 계산)하기 좋게 펼치는 함수 입력 데이터에서 필터를 적용하는 영역을 한 줄로 늘어 놓는다. 필터를 적용하는 영역이 겹쳐서 메모리를 더 많이 사용하지만, 선형 대수 라이브러리가 행렬 계산을 매우 빠르게 처리해줘서 속도에서는 이점이 있다. Affine 계층에서 한 것과 유사한 계산 마지막에 2차원 출력 데이터를 4차원으로 변형(reshape)7.4.3 합성곱 계층 구현하기# 필터 크기, 스트라이드, 패딩을 고려해서 입력 데이터를 2차원 배열로 전개def im2col(input_data, filter_h, filter_w, stride=1, pad=0): \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화). Parameters ---------- input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비) filter_h : 필터의 높이 filter_w : 필터의 너비 stride : 스트라이드 pad : 패딩 Returns ------- col : 2차원 배열 \"\"\" N, C, H, W = input_data.shape out_h = (H + 2*pad - filter_h)//stride + 1 out_w = (W + 2*pad - filter_w)//stride + 1 # np.pad(array, pad_width, mode, **kwargs) # array: 패딩할 배열 # pad_width: 각 축마다 패딩할 값의 수 # mode: 패딩 방식 # default로 0으로 패딩 # 신경망에서 패딩을 하게 되면 이미지와 채널 수에 해당되는 차원은 하지 않기 때문에 pad_width에 해당되는 # 인수의 처음과 두번째 원소가 (0,0)인 것이다. 즉, 해당 차원은 패딩하지 않는다. img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant') col = np.zeros((N, C, filter_h, filter_w, out_h, out_w)) # Q. for 문 이해 안됨 for y in range(filter_h): y_max = y + stride*out_h for x in range(filter_w): x_max = x + stride*out_w col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride] col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1) return colimport sys, ossys.path.append(os.pardir)from common.util import im2colx1 = np.random.rand(1, 3, 7, 7)col1 = im2col(x1, 5, 5, stride=1, pad=0)print(col1.shape)(9, 75)x2 = np.random.rand(10, 3, 7, 7)col2 = im2col(x2, 5, 5, stride=1, pad=0)print(col2.shape)(90, 75)class Convolution: def __init__(self, W, b, stride=1, pad=0): self.W = W self.b = b self.stride = stride self.pad = pad def forward(self, x): FN, C, FH, FW = self.W.shape N, C, H, W = x.shape out_h = int(1 + (H + 2*self.pad - FH) / self.stride) out_w = int(1 + (W + 2*self.pad - FW) / self.stride) col = im2col(x, FH, FW, self.stride, self.pad) col_W = self.W.shape(FN, -1). T out = np.dot(col, col_W) + self.b out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) return out역전파에서는 im2col을 반대로 처리하는 col2im을 사용한다.def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0): \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다. Parameters ---------- col : 2차원 배열(입력 데이터) input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)） filter_h : 필터의 높이 filter_w : 필터의 너비 stride : 스트라이드 pad : 패딩 Returns ------- img : 변환된 이미지들 \"\"\" N, C, H, W = input_shape out_h = (H + 2*pad - filter_h)//stride + 1 out_w = (W + 2*pad - filter_w)//stride + 1 col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2) img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1)) for y in range(filter_h): y_max = y + stride*out_h for x in range(filter_w): x_max = x + stride*out_w img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :] return img[:, :, pad:H + pad, pad:W + pad]7.4.4 풀링 계층 구현하기합성곱 계층과 마찬가지로 im2col 함수를 이용한다. 다만, 다른 점은 채널마다 독립적으로 전개한다는 점이다.class Pooling: def __init__(self, pool_h, pool_w, stride=1, pad=0): self.pool_h = pool_h self.pool_w = pool_w self.stride = stride self.pad = pad def forward(self, x): N, C, H, W = x.shape out_h = int(1 + (H - self.pool_h) / self.stride) out_w = int(1 + (W - self.pool_w) / self.stride) # 전개 (1) col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) col = col.reshape(-1, self.pool_h*self.pool_w) # 최댓값 (2) out = np.max(col, axis=1) # 성형 (3) out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) return out7.5 CNN 구현하기손글씨 숫자 인식하는 CNN 구현 구조: conv-relu-pooling-affine-relu-affine-softmaxSimpleConvNet 클래스 __init__ 인수 input_dim: 입력 데이터(채널 수, 높이, 너비)의 차원 conv_param: 합성곱 계층의 하이퍼파라미터(딕셔너리) filter_num: 필터 수 filter_size: 필터 크기 stride: 스트라이드 pad: 패딩 hidden_size: 은닉층의 뉴런 수 output_size: 출력층의 뉴런 수 weight_init_std: 초기화 때의 가중치 표준편차 class SimpleConvNet: def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, hidden_size=100, output_size=10, weight_init_std=0.01): filter_num = conv_param['filter_num'] filter_size = conv_param['filter_size'] filter_pad = conv_param['pad'] filter_stride = conv_param['stride'] input_size = input_dim[1] conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1 pool_output_size = int((filter_num * (conv_output_size/2) * (conv_output_size/2))) self.params = {} self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size) self.params['b1'] = np.zeros(filter_num) self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size) self.params['b2'] = np.zeros(hidden_size) self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size) self.params['b3'] = np.zeros(output_size) self.layers = OrderedDict() self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad']) self.layers['Relu1'] = Relu() self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2) self.layers['Affine1'] = Affine(self.parmas['W2'], self.params['b2']) self.layers['Relu2'] = Relu() self.laeyrs['Affine2'] = Affine(self.params['W3'], self.parmas['b3']) self.last_layer = SoftmaxWithLoss() def predict(self, x): for layer in self.layers.values(): x = layers.forward(x) return x def loss(self, x, t): y = self.predict(x) return self.last_layer.forward(y, t) def gradient(self, x, t): # 순전파 self.loss(x, t) # 역전파 dout = 1 dout = self.last_layer.backward(dout) layers = list(self.layers.values()) layers.reverse() for layer in layers: dout = layer.backward(dout) # 결과 저장 grads = {} grads['W1'] = self.layers['Conv1'].dW grads['b1'] = self.layers['Conv1'].db grads['W2'] = self.layers['Affine1'].dW grads['b2'] = self.layers['Affine1'].db grads['W3'] = self.layers['Affine2'].dW grads['b3'] = self.layers['Affine'].db return grads# coding: utf-8import sys, ossys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import numpy as npimport matplotlib.pyplot as pltfrom dataset.mnist import load_mnistfrom simple_convnet import SimpleConvNetfrom common.trainer import Trainer# 데이터 읽기(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)# 시간이 오래 걸릴 경우 데이터를 줄인다.#x_train, t_train = x_train[:5000], t_train[:5000]#x_test, t_test = x_test[:1000], t_test[:1000]max_epochs = 20network = SimpleConvNet(input_dim=(1,28,28), conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1}, hidden_size=100, output_size=10, weight_init_std=0.01) trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=max_epochs, mini_batch_size=100, optimizer='Adam', optimizer_param={'lr': 0.001}, evaluate_sample_num_per_epoch=1000)trainer.train()# 매개변수 보존network.save_params(\"params.pkl\")print(\"Saved Network Parameters!\")# 그래프 그리기markers = {'train': 'o', 'test': 's'}x = np.arange(max_epochs)plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)plt.xlabel(\"epochs\")plt.ylabel(\"accuracy\")plt.ylim(0, 1.0)plt.legend(loc='lower right')plt.show()train loss:2.2995260237545354=== epoch:1, train acc:0.131, test acc:0.118 ===train loss:2.2975321648494162train loss:2.294610007994603train loss:2.287643742679368train loss:2.277571750143067train loss:2.273888829395921train loss:2.25138555721259train loss:2.231636338533291.....train loss:0.0007504085776238781train loss:0.0006305754900100426train loss:3.650351210427843e-05train loss:0.00011660007186879213train loss:0.0034911552723316595train loss:2.7817370077523596e-05train loss:0.00278068868231808train loss:0.00860630414349712train loss:0.000212820545808942train loss:0.0041047099315977015train loss:0.0023794835160068843train loss:0.0026915451634738257train loss:0.00011453829850605189train loss:0.0023212412777749184train loss:4.672228737306881e-05train loss:0.0005770183132382689train loss:3.998510631355986e-05train loss:0.0012251012653243113train loss:0.001225842621761431train loss:0.00018016703930926606train loss:0.0011863858625609006train loss:0.00020601261406510895train loss:0.020417215290379324train loss:0.00020197674243562533train loss:0.006390657381187902train loss:0.00010603314122986489train loss:0.00037329694319883177train loss:0.0036877658448098848train loss:0.0006094631970039707train loss:0.00047120910979439585train loss:0.00019528056770125042train loss:0.0001959878676012942train loss:0.0003382110604612978train loss:0.0015579024218455577train loss:0.0026843087959261287train loss:0.013432609885665527train loss:0.001941956277535325train loss:0.0001861940532349634train loss:0.0027816507925776707train loss:0.001876821975816105train loss:0.0008053180855204114train loss:0.0022498999334715497train loss:0.0004023503025203104train loss:0.002829371045887539train loss:0.006198255683125497train loss:0.0013390517584302966train loss:0.0016573133599115631train loss:0.001934521872157699train loss:0.0010795976011183056=============== Final Test Accuracy ===============test acc:0.9863Saved Network Parameters!7.6 CNN 시각화하기7.6.1 1번째 층의 가중치 시각화하기# coding: utf-8import numpy as npimport matplotlib.pyplot as pltfrom simple_convnet import SimpleConvNetdef filter_show(filters, nx=8, margin=3, scale=10): \"\"\" c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py \"\"\" FN, C, FH, FW = filters.shape ny = int(np.ceil(FN / nx)) fig = plt.figure() fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) for i in range(FN): ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[]) ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest') plt.show()network = SimpleConvNet()# 무작위(랜덤) 초기화 후의 가중치filter_show(network.params['W1'])# 학습된 가중치network.load_params(\"params.pkl\")filter_show(network.params['W1'])무엇을 보고 있는 것일까? 에지(색상이 바뀐 경계선), 블롭(국소적으로 덩어리진 영역) 등 예: 세로 에지(필터1)과 가로 에지(필터2)에 반응하는 필터 초기 계층에서 필터는 원시적인 정보를 추출한다.7.6.2 층 깊이에 따른 추출 정보 변화 계층이 깊어질수록 추출되는 정보(강하게 반응하는 뉴런)는 더 추상화된다. 예: AlexNet 일반 사물 인식 8층 CNN 마지막 층은 완전연결 계층 1층: 에지와 블롭, 3층 텍스쳐, 5층: 사물의 일부, 마지막층: 사물의 클래스에 주로 반응 깊어질 수록 사물의 의미를 이해하도록 변화 7.7 대표적인 CNN7.7.1 LeNet CNN의 원조 손글씨 숫자 인식 네트워크 합성곱과 서브 샘플링 계층 반복 현재는 서브 샘플링 대신 최대 풀링 서브 샘플링은 2X2 필터로 average pooling 마지막 층: 완전연결 계층 활성화 함수: sigmoid 현재는 주로 ReLU 함수 사용 7.7.2 AlexNet 딥러닝 돌풍의 주역 LeNet과 큰 구조에서는 유사 변경점 활성화 함수: ReLU LRN(local response normalization): 국소적 정규화 계층 드롭아웃 딥러닝의 발전 원동력은? 빅데이터 GPU" }, { "title": "Sample_post", "url": "/posts/sample_post/", "categories": "ML", "tags": "", "date": "2022-01-02 00:00:00 +0900", "snippet": "\\[\\left[\\begin{matrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\\\end{matrix}\\right]\\]\\[\\left[\\begin{array}{cc} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\\\end{array}\\right]\\]$ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} $\\[\\lim_{x\\to 0}{\\frac{e^x-1}{2x}}\\overset{\\left[\\frac{0}{0}\\right]}{\\underset{\\mathrm{H}}{=}}\\lim_{x\\to 0}{\\frac{e^x}{2}}={\\frac{1}{2}}\\]" }, { "title": "Oversampling", "url": "/posts/Oversampling/", "categories": "Project", "tags": "subject_project", "date": "2021-12-04 00:00:00 +0900", "snippet": "Project-OversamplingClassification for Imabalanced Data머신러닝 분류(classifier) 문제에서 target 변수의 범주가 불균형(imbalance) 할 경우model의 학습 과정에서 major한 class의 데이터의 학습이 주로 이루어지므로 minor한 class의학습이 제대로 이루어지 않는다.이러한 imbalanced data의 경우 실제로 자주 마주할 수 있는 데이터라고 할 수 있고,실질적으로 minor class를 잘 맞추는 것이 현실에서 주요한 머신러닝 과제라고 할 수 있다.ex) 특정 질병을 예측하는 문제의 경우 질병을 가진 사람의 데이터가 소수 class로 이루어질 가능성이 크고, 이때 정상인지를 예측하는 것보다질병을 예측하는 것에 focus를 맞추게 된다.classification of evaluation본격적으로 머신러닝 모델에서 불균형 데이터를 처리하는 기법에 대해 설명하기에 앞서 분류 모델에서 사용되는 평가 지표에 대해알아보고자 한다. 편의상 Binary classification 문제를 가정한다.Confusion-matrix(혼동 행렬)혼동행렬은 머신러닝 분류 평가지표로 많이 활용되는 행렬으로 각 행은 실제 값을 각 열은 분류 모델에서의 예측값을 의미한다Accuracy(정확도)정확도는 전체 데이터 中 예측에 성공한 데이터의 비율을 의미한다.$ Accuracy = \\frac{TP+TN}{TP+FP+TN+FN} $이 때 class가 고르게 분포되어 있는 경우 정확도를 모델 평가 지표로 사용하는 것이 좋지만 class가 불균형한 형태의 경우정확도는 해당 모델이 좋은 모델인지에 대한 올바른 판단을 제시해 줄 수 없다.why? True와 False의 비율이 9:1이라 할 때 단순히 True라고 예측하기만 해도 90%의 정확도를 얻게 된다.따라서 Imbalanced한 데이터의 경우 다른 지표를 고려해야 한다.Precision(정밀도)예측 결과가 positive일 때 실제로 positive인 경우를 말한다.$ Precision = \\frac{TP}{TP + FP} $Sensitivity(민감도 = 재현율)실제로 positive일 때 예측도 positive인 경우를 의미한다.$ Sensitivity = \\frac{TP}{TP+FN} $Specificity(특이도)현실이 부정일 때 예측도 부정인 경우를 의미한다.$ Specificity = \\frac{TN}{TN+FP} $F1-SCORE정밀도와 재현율은 일반적으로 trade-off 관계를 갖는다. 따라서 이 두 개의 지표를 모두 고려한 지표가 f1-score이고정밀도와 재현율의 조화평균으로 표현된다.민감도를 P 재현율을 R이라고 하면$ f1-score = 2 \\frac{PR}{P+R} $ROC-CURVEroc 커브는 가로축에는 거짓 긍정율 새로축에는 민감도가 배치가 되고 각각의 값들은 [0,1]의 범위를 갖는다.이 때 cut-off-value(Threshold 라고도 함)의 변화에 따른 거짓 긍정율과 민감도 값을 그래프로 그린 것이 roc-curve이다.$ y=x $는 임의의 분류 모델 즉 동전 던지기와 같으며 (0,1)의 점에 수렴할 수록 좋은 분류 모델을 의미하게 된다.이 때 이 roc-curve의 면적을 AUC라고 하며 불균형한 데이터의 분류 모델의 지표로 많이 활용된다.How to modeling Imbalanced data일반적으로 불균형 데이터를 모델링하는 방법은 크게 3가지가 존재한다.under sampling이 방법의 경우 다수의 데이터를 소수의 데이터의 개수와 유사하게 맞추기 위해 다수의 데이터를 삭제하는 작업을 말한다.이 경우 데이터의 개수가 줄어들기 때문에 다량의 loss-information이 발생한다는 단점이 있지만데이터의 개수를 줄임에 따라 모델링 시간이 단축된다는 장점도 있다.oversampling이 방법의 경우 소수의 데이터를 특정 알고리즘에 의해 합성 데이터를 생성하여 다수의 데이터와 비슷한 데이터의 개수를 가지게만드는 것을 의미한다. 이 경우 under-sampling 방법과는 달리 정보의 손실이 일어나지 않는 반면에 데이터를 늘리기 때문에모델링에 걸리는 시간이 늘어난다는 단점이 존재한다.Loss-function approach이 방법의 경우 데이터를 늘리거나 줄이지 않고 모델링에서의 loss-function을 조정하는 방법이다.일반적인 손실 함수의 경우 모든 데이터에 대해 동일한 가중치를 부여하여 계산하는 반면불균형 데이터의 경우 소수 class 데이터를 예측하는 것에 틀리는 것에 좀 더 가중치를 부여하여모델의 소수 class 데이터에 대해 좀 더 잘 예측하도록 만든다.본 포스팅에서는 oversampling 기법과 이를 python으로 활용하는 방법에 대해 다뤄보고자 한다.undersampling 과 oversamplingDATA-SET : Kaggle Loan prediction based on customer behaviorSMOTE가장 대표적인 oversampling 방법이다. 대다수의 oversampling 기법이이 SMOTE를 응용한 방법이라고도 할 수 있다. SMOTE에서 합성 데이터를 생성하는 방식은 다음과 같다. 임의의 소수 데이터를 잡는다. 해당 소수 데이터로부터 가장 가까운 k개의 이웃 데이터를 잡는다. 기준이 되는 소수 데이터와 이웃이 되는 데이터 사이의 직선 상에서 합성 데이터를 랜덤하게 생성한다. 이를 그림으로 표현하면 다음과 같다.import numpy as npimport pandas as pdfrom imblearn.over_sampling import SMOTEimport timefrom sklearn.preprocessing import LabelEncoderdf =pd.read_csv('C:/Users/koyounghun/Desktop/찌르레기/Loan_data/Loan_train.csv')df = df.drop('Id',axis = 1)labelEncoder = LabelEncoder()data = dffor e in data.columns: if data[e].dtype == 'object': labelEncoder.fit(list(data[e].values)) data[e] = labelEncoder.transform(data[e].values) # Accommodate the data that has been changed df = data y = df.Risk_FlagX = df.drop('Risk_Flag', axis=1)import warningswarnings.filterwarnings(action='ignore')df['Risk_Flag'].value_counts() # 전체 데이터 중 약 10%만이 1로 코딩 0 2210041 30996Name: Risk_Flag, dtype: int64# smote 적용start = time.time()smote = SMOTE(random_state = 101)X_smote, y_smote = smote.fit_resample(X, y)end = time.time()print(f\"{end - start:.5f} sec\")np.unique(y_smote, return_counts = True) # 동일한 개수로 oversampling 되었음3.35296 sec(array([0, 1], dtype=int64), array([221004, 221004], dtype=int64))ADASYNSMOTE를 개선한 방법이다. SMOTE의 경우 모든 소수 데이터에 대해 동일한 개수의 합성 데이터를 생성하는 반면에ADASYN 기법은 주변의 다수 데이터를 고려하여 해당 데이터에서 생성할 데이터의 개수를 계산하고 이를 이용하여합성 데이터를 생성하는 방법이다.즉 주변 데이터의 밀도에 따라 데이터를 생성하는 방법이라고 할 수 있다.from imblearn.over_sampling import ADASYNstart = time.time()adasyn = ADASYN(random_state = 101)X_over_ada, y_over_ada = adasyn.fit_resample(X, y)end = time.time()np.unique(y_over_ada, return_counts = True) # 동일한 개수로 oversampling 되었음print(f\"{end - start:.5f} sec\")9.90477 secDistribution SMOTE합성 데이터를 생성하는 방법은 다음과 같다. 합성 데이터의 개수를 정한다. (보통 1:1의 비율을 맞춘다.) 예를 들어 소수 데이터가 10개, 다수의 데이터가 100개라면 합성할 데이터의 개수는 90개 $ k = int(S_{syn} / S_{min})$ 을 결정 이 때 $S_{min}$ 은 소수 클래스 데이터의 개수를 의미 각 $x_i \\in S_{min}$에 대해 같은 클래스의 평균 거리와 다른 클래스간의 평균거리를 계산한다. $ \\alpha = \\overline{d_{intra}} / \\overline{d_{extra}} $ 를 계산한다. 이 때 이 값이 0.5보다 작으면 smote와 같이 합성 데이터 생성 이 값이 0.5보다 크면 합성 데이터를 생성하지 않는다. import smote_variants as svoversampler= sv.NDO_sampling(random_state = 101, n_jobs = -1)start = time.time()X_samp, y_samp = oversampler.sample(np.array(X), np.array(y))end = time.time()print(f\"{end - start:.5f} sec\")2021-12-04 18:06:33,085:INFO:NDO_sampling: Running sampling via ('NDO_sampling', \"{'proportion': 1.0, 'n_neighbors': 5, 'T': 0.5, 'n_jobs': -1, 'random_state': 101}\")I1204 18:06:33.085295 20816 _smote_variants.py:17104] NDO_sampling: Running sampling via ('NDO_sampling', \"{'proportion': 1.0, 'n_neighbors': 5, 'T': 0.5, 'n_jobs': -1, 'random_state': 101}\")18.65919 secnp.unique(y_samp, return_counts = True) # 동일한 개수로 oversampling 되었음(array([0, 1], dtype=int64), array([221004, 221004], dtype=int64))위와 같이 oversampling된 데이터에 대해서 train을 실시하고 test-set으로 모델의 최종적인 성능을 평가하면 된다.이 때 주의할 점은 test-set의 경우 oversampling이나 undersampling 기법을 적용하면 안된다는 점이다.추가적으로 앞서 소개한 3가지 기법 이외에도 boderline smote, smote-bagging등과 같이 여러 smote의 응용 방법과rusboost와 같이 undersampling을 적용한 기법도 존재하며 oversampling과 undersampling을 결합한 hybrid 방법이 존재Loan Data를 이용해 train/test 7:3의 비율로 분할한 후 5-fold-cross validation을 적용하여 데이터 분석을 진행각각 LDA/QDA/KNN/Random-forest/XGBoost/Light-GBM/Decision-tree 모델을 적용하였다.평가 지표 선정def get_clf_eval(y_test, y_pred): confmat=pd.DataFrame(confusion_matrix(y_test, y_pred), index=['True[0]', 'True[1]'], columns=['Predict[0]', 'Predict[1]']) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) AUC = roc_auc_score(y_test, y_pred) g_means = geometric_mean_score(y_test, y_pred) print(confmat) print(\"\\n정확도 : {:.3f} \\n정밀도 : {:.3f} \\n재현율 : {:.3f} \\nf1-score : {:.3f} \\nAUC : {:.3f} \\n기하평균 : {:.3f} \\n\".format(accuracy, precision, recall, f1, AUC, g_means)) 모델 적합 (Non-oversampling)from sklearn.linear_model import LogisticRegressionfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysisfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifierimport xgboost as xgbfrom xgboost.sklearn import XGBClassifierfrom sklearn.neural_network import MLPClassifierfrom lightgbm import LGBMClassifierimport timemodels_X = []models_X.append(('LDA', LinearDiscriminantAnalysis())) # LDA 모델models_X.append(('QDA', QuadraticDiscriminantAnalysis())) # QDA 모델models_X.append(('KNN', KNeighborsClassifier())) # KNN 모델models_X.append(('DT', DecisionTreeClassifier())) # 의사결정나무 모델models_X.append(('RF', RandomForestClassifier())) # 랜덤포레스트 모델models_X.append(('XGB', XGBClassifier())) # XGB 모델models_X.append(('Light_GBM', LGBMClassifier())) # Light_GBM 모델for name, model in models_X: start = time.time() model.fit(X_train, y_train) end = time.time() - start msg = \"%s - train_score : %.3f, test score : %.3f, time : %.5f 초\" % (name, model.score(X_train, y_train), model.score(X_test, y_test), end) print(msg)SMOTE 적용from sklearn.model_selection import train_test_splitX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_s, y_s, test_size = 0.3, random_state = 101)models_s = []models_s.append(('LDA', LinearDiscriminantAnalysis())) # LDA 모델models_s.append(('QDA', QuadraticDiscriminantAnalysis())) # QDA 모델models_s.append(('KNN', KNeighborsClassifier())) # KNN 모델models_s.append(('DT', DecisionTreeClassifier())) # 의사결정나무 모델models_s.append(('RF', RandomForestClassifier())) # 랜덤포레스트 모델models_s.append(('XGB', XGBClassifier())) # XGB 모델models_s.append(('Light_GBM', LGBMClassifier(boost_from_average=False))) # Light_GBM 모델for name, model in models_s: start = time.time() model.fit(X_train_s, y_train_s) end = time.time() - start msg = \"%s - train_score : %.3f, test score : %.3f, time : %.5f 초\" % (name, model.score(X_train_s, y_train_s), model.score(X_test_s, y_test_s), end) print(msg) # 모델 갯수a = list(range(0,len(models_s)))for i in a: print(\"----------SMOTE + %s 모델 적용----------\" % (models_s[i][0])) get_clf_eval(y_test_s, models_s[i][1].predict(X_test_s))이 외에도 ADASYN,Distribution-smote 방법도 동일하게 적용하면 된다.앞서 설명한 바와 같이 accuracy의 경우 oversampling을 적용한 결과 정확도의 경우 기존에 비해 떨아지지만AUC의 경우 성능이 상승하였음 다음은 grid-search를 통한 간단한 튜닝 작업을 진행Tunning using Pipeline다음은 ADASYN과 KNN을 결합하여 모델의 hyperparameter를 조정하기로 하였음start = time.time()pipeline = Pipeline(steps= [(\"ADASYN\", ADASYN(random_state = 101)), (\"KNN\", KNeighborsClassifier()) ])param_grid = { \"ADASYN__sampling_strategy\": [0.5,0.75,1], \"KNN__n_neighbors\": list(range(10,101,10)) }gs_pipeline_1 = GridSearchCV(pipeline, param_grid=param_grid, verbose=2, scoring=make_scorer(roc_auc_score), cv = 5)gs_pipeline_1.fit(X_train, y_train)end = time.time()print(f\"{end - start:.5f} sec\")# Store the best modelada_best_model = gs_pipeline_1.best_estimator_y_validation_preds = ada_best_model.predict(X_test)roc_auc_score(y_test, y_validation_preds)scores_df = pd.DataFrame(gs_pipeline_1.cv_results_)추가적으로 다른 모델들도 튜닝 작업을 진행하였음결과는 다음과 같다.AUC의 성능의 경우 Random-forest의 모델이 가장 좋았지만 computation-time이 굉장히 오래 걸렸음을 확인할 수 있음그에 비해 Light-gbm의 경우 Random-forest에 비해서는 성능이 약간 떨어지지만, computation-time이 비약적으로 줄어들었음을 확인할 수 있음Grid-search Results of KNN낮은 성능과 높은 성능을 보이는 일정한 영역이 존재함을 확인=&gt; 추가적으로 K의 개수를 10 이하로, oversampling-proportion을 1에 근접한 값으로 탐색한다면 안정적인 성능을 보일 것으로 기대Conclusion 불균형 데이터의 경우 정확도보다 AUC/f1-socre 등의 지표를 선정하는 것이 바람직함 불균형 데이터를 처리하기 위한 방법으로는 undersampling/oversampling/loss-function apporoach 등 3가지 존재 oversampling 기법을 적용한 결과 AUC 지표가 상승함을 확인할 수 있었음 즉 oversampling 기법이 불균형 데이터를 처리하는 타당함을 확인할 수 있었음 " }, { "title": "밑바닥부터 시작하는 딥러닝 Chapter06", "url": "/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter06/", "categories": "ML, book_study", "tags": "Deep_learning", "date": "2021-11-25 00:00:00 +0900", "snippet": "Chapter 6 학습 관련 기술들딥러닝 학습 효율, 정확도 개선 방법 가중치 매개변수 최적값 탐색하는 최적화 방법 가중치 매개변수 초깃값 하이퍼파라미터 설정 방법 오버피팅 대응책 가중치 감소 드롭아웃 배치 정규화6.1 매개변수 갱신신경망 학습의 목표는 매개변수의 최적값을 찾는 최적화(optimization)이다. 여태까지 사용한 최적화 방법으로 미분을 이용했는데 이 방법을 확률적 경사 하강법(SGD)라고 한다.6.1.1 모험가 이야기 (생략)6.1.2 확률적 경사 하강법(SGD) SGD 수식\\[W \\leftarrow W - \\eta \\frac {\\partial L} {\\partial W}\\]class SGD: def __init__(self, lr=0.01): self.lr = lr def update(self, params, grads): for key in params.keys(): params[key] -= self.lr * grads[key]다음은 SGD 클래스를 사용하는 의사코드 예시이다.network = TwoLayerNet(...)optimizer = SGD()for i in range(10000): ... x_batch, t_batch = get_mini_batch(...) grads = network.gradient(x_batch, t_batch) params = network.params optimizer.update(params, grads) ...매개 변수 갱신은 optimizer가 책임을 지고 optimizer에 매개변수와 기울기 정보만 넘겨주면 된다. 여러 최적화 관련 클래스를 만들어 모듈화하면 다양한 방법으로 쉽게 최적화할 수 있다.6.1.3 SGD의 단점 예시 함수\\[f(x, y) = \\frac 1 {20} x^2 + y^2\\]▲ 그래프(왼쪽), 등고선(오른쪽)▲ 기울기최솟값이 되는 위치는 $(x, y) = (0, 0)$이지만 대부분의 기울기는 $(0, 0)$을 가리키지 않는다.▲ SGD에 의한 최적화 갱신 경로 비등방성 함수(방향에 따라 기울기가 달라지는 함수)에서 탐색 경로가 비효율적 근본 원인은 최솟값과 다른 방향을 가리키는 기울기6.1.4 모멘텀(Momentum)\\(\\mathbf{v} \\leftarrow \\alpha \\mathbf{v} - \\eta \\frac {\\partial L} {\\partial W}\\)\\(W \\leftarrow W + \\mathbf{v}\\) $\\mathbf{v}$: 속도 위의 첫번째 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타낸다. $\\alpha\\mathbf{v}$: 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할import numpy as npclass Momentum: def __init__(self, lr=0.01, momentum=0.9): self.lr = lr self.momentum = momentum self.v = None def update(self, params, grads): if self.v is None: self.v = {} for key, val in params.items(): self.v[key] = np.zeros_like(val) for key in params.keys(): self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] params[key] += self.v[key]▲ 모멘텀에 의한 최적화 갱신 경로Q: 모멘텀 왜 이런 식으로 움직이는지?SGD와 비교하면 지그재그가 덜 하다. 이유는 $x$축의 힘은 작지만 방향이 변하지 않아서 일정하게 가속하고 $y$축의 힘은 크지만 위아래로 번갈아 받아서 상충하여 $y$축 방향의 속도는 안정적이지 않기 때문이다.6.1.5 AdaGrad학습률 감소(learning rate decay)학습을 진행하면서 학습률을 점차 줄여나가는 방법AdaGrad는 개별 매개 변수에 적응적으로 학습률을 조정하면서 학습을 진행한다.\\[\\mathbf{h} \\leftarrow \\mathbf{h} + \\frac {\\partial L} {\\partial \\mathbf{W}} \\odot \\frac {\\partial L} {\\partial \\mathbf{W}}\\]\\[\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac 1 {\\sqrt{\\mathbf{h}}} \\frac {\\partial L} {\\partial \\mathbf{W}}\\] $\\odot$: 행렬 원소별 곱셈 $\\mathbf{h}$: 기존 기울기 제곱해서 계속 더해주고 매개변수 갱신할 때 $\\frac 1 {\\sqrt{\\mathbf{h}}}$로 학습률 조정 많이 움직인 원소의 학습률이 낮아짐AdaGrad는 과거의 기울기의 제곱을 계속 누적해서 어느 순간 갱신량이 0이 되는 문제가 발생한다. 이를 개선한 기법으로 RMSProp이 있다. RMSProp은 지수이동평균을 이용해서 과거의 기울기를 서서히 잊고 새로운 기울기 정보를 더욱 크게 반영한다.class AdaGrad: def __init__(self, lr=0.01): self.lr = lr self.h = None def update(self, params, grads): if self.h is None: self.h = {} for key, val in parmas.items(): self.h[key] = np.zeros_like(val) for key in params.keys(): self.h[key] += grads[key] * grads[key] params[key] -= self.lr * gras[key] / (np.sqrt(self.h[key]) + 1e-7)1e-7을 더해주는 이유는 0으로 나누는 일을 막기 위함이다. 대분분의 딥러닝 프레임워크에서 이 값도 인수로 설정할 수 있다.▲ AdaGrad에 의한 최적화 갱신 경로$y$축 방향은 기울기가 처음엔 크게 움직이지만, 이에 비례해서 갱신 정도도 빠르게 감소하기 때문에 지그재그 움직임이 줄어든다.6.1.6 Adam 모멘텀과 AdaGrad를 융합한 것 같은 방법 편향 보정▲ Adam에 의한 최적화 갱신 경로Adam의 하이퍼파라미터 학습률 $\\alpha$ 일차 모멘텀용 계수 $\\beta_1$: 0.9(기본값) 이차 모멘텀용 계수 $\\beta_2$: 0.999(기본값)6.1.7 어느 갱신 방법을 이용할 것인가?위의 그래프들을 비교했을 때 AdaGrad가 가장 좋을 것 같지만 문제에 따라 다르다. 상황에 맞게 선택하는 것이 중요하다.6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교# coding: utf-8import osimport syssys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import matplotlib.pyplot as pltfrom dataset.mnist import load_mnistfrom common.util import smooth_curvefrom common.multi_layer_net import MultiLayerNetfrom common.optimizer import *# 0. MNIST 데이터 읽기==========(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)train_size = x_train.shape[0]batch_size = 128max_iterations = 2000# 1. 실험용 설정==========optimizers = {}optimizers['SGD'] = SGD()optimizers['Momentum'] = Momentum()optimizers['AdaGrad'] = AdaGrad()optimizers['Adam'] = Adam()#optimizers['RMSprop'] = RMSprop()networks = {}train_loss = {}for key in optimizers.keys(): networks[key] = MultiLayerNet( input_size=784, hidden_size_list=[100, 100, 100, 100], output_size=10) train_loss[key] = [] # 2. 훈련 시작==========for i in range(max_iterations): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] for key in optimizers.keys(): grads = networks[key].gradient(x_batch, t_batch) optimizers[key].update(networks[key].params, grads) loss = networks[key].loss(x_batch, t_batch) train_loss[key].append(loss) if i % 100 == 0: print( \"===========\" + \"iteration:\" + str(i) + \"===========\") for key in optimizers.keys(): loss = networks[key].loss(x_batch, t_batch) print(key + \":\" + str(loss))===========iteration:1800===========SGD:0.2273494384423423Momentum:0.0831109822073276AdaGrad:0.030900737592896608Adam:0.037067473804141174===========iteration:1900===========SGD:0.1857942076948705Momentum:0.06184418686111305AdaGrad:0.05637063636435206Adam:0.0883517703401202# 3. 그래프 그리기==========markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}x = np.arange(max_iterations)for key in optimizers.keys(): plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)plt.xlabel(\"iterations\")plt.ylabel(\"loss\")plt.ylim(0, 1)plt.legend()plt.show()6.2 가중치의 초깃값6.2.1 초깃값을 0으로 하면?가중치 감소(weight decay) 가중치 매개변수 값이 작아지도록 학습하는 방법 오버피팅 억제 효과가중치 초깃값을 0으로(균일한 값으로) 시작하면? 올바른 학습 X 오차역전파법에서 모든 가중치 값이 똑같이 갱신되기 때문에 해결: 초깃값을 무작위로 설정6.2.2 은닉층의 활성화값 분포시그모이드 함수를 활성화 함수로 사용하는 5층 신경망을 통해 실험 각 층의 뉴런은 100개 입력 데이터 수는 1000개 표준편차가 1인 정규분포 이용import numpy as npimport matplotlib.pyplot as pltdef sigmoid(x): return 1 / (1 + np.exp(-x))x = np.random.randn(1000, 100)node_num = 100hidden_layer_size = 5activations = {}for i in range(hidden_layer_size): if i != 0: x = activations[i-1] w = np.random.randn(node_num, node_num) * 1 a = np.dot(x, w) z = sigmoid(a) activations[i] = zplt.figure(figsize=(20,5))for i, a in activations.items(): plt.subplot(1, len(activations), i+1) plt.title(str(i+1) + \"-layer\") plt.hist(a.flatten(), 30, range=(0,1))plt.show() 활성화 값의 분포가 0, 1에 치우쳐져 있다. 시그모이드 함수는 0, 1에 가까워질 수록 미분값이 0에 가까워진다. 미분값이 0에 가까워지면 더 이상 갱신이 일어나지 않는 기울기 소실(gradient vanishing) 문제가 발생한다.x = np.random.randn(1000, 100)node_num = 100hidden_layer_size = 5activations = {}for i in range(hidden_layer_size): if i != 0: x = activations[i-1] w = np.random.randn(node_num, node_num) * 0.01 a = np.dot(x, w) z = sigmoid(a) activations[i] = zplt.figure(figsize=(20,5))for i, a in activations.items(): plt.subplot(1, len(activations), i+1) plt.title(str(i+1) + \"-layer\") plt.hist(a.flatten(), 30, range=(0,1))plt.show()activations{0: array([[0.47510529, 0.5337012 , 0.50246425, ..., 0.53564721, 0.51806879, 0.52973781], [0.48389798, 0.49156359, 0.5004307 , ..., 0.47501203, 0.50423051, 0.481287 ], [0.45297996, 0.49237966, 0.47288158, ..., 0.45614095, 0.44685061, 0.50093314], ..., [0.48098354, 0.50503036, 0.50289379, ..., 0.48413844, 0.5159619 , 0.51706842], [0.50609917, 0.54160537, 0.47694298, ..., 0.50507099, 0.50068575, 0.48808848], [0.52785112, 0.50274475, 0.50429485, ..., 0.48173126, 0.52671102, 0.49431768]]), 1: array([[0.48001208, 0.47977019, 0.50622738, ..., 0.48648041, 0.52498678, 0.5052194 ], [0.48087855, 0.48119919, 0.50589301, ..., 0.48710548, 0.524929 , 0.50659169], [0.48089019, 0.47950954, 0.50513487, ..., 0.48614814, 0.52541495, 0.5052187 ], ..., [0.48043455, 0.48054057, 0.50529645, ..., 0.48716627, 0.52480393, 0.50565974], [0.48068606, 0.47960207, 0.5055231 , ..., 0.48589233, 0.52555305, 0.50566242], [0.48097261, 0.48197977, 0.50566901, ..., 0.48599371, 0.5252626 , 0.50501786]]), 2: array([[0.50681215, 0.5059925 , 0.47340957, ..., 0.48943754, 0.49200943, 0.50938571], [0.50682023, 0.50598354, 0.47340571, ..., 0.48944364, 0.49201049, 0.50935569], [0.50681171, 0.50596828, 0.4734341 , ..., 0.48948501, 0.49200742, 0.50937377], ..., [0.50680604, 0.50599338, 0.47339008, ..., 0.48944666, 0.49202149, 0.50936353], [0.50679272, 0.50598412, 0.47341447, ..., 0.48947331, 0.4920083 , 0.50937677], [0.50685574, 0.50597987, 0.47342398, ..., 0.48945713, 0.49202442, 0.50937627]]), 3: array([[0.50155717, 0.49433578, 0.50044248, ..., 0.51843858, 0.50104222, 0.50660993], [0.50155707, 0.4943363 , 0.50044235, ..., 0.51843929, 0.50104243, 0.50661035], [0.5015564 , 0.49433615, 0.50044264, ..., 0.51843894, 0.50104258, 0.50661065], ..., [0.5015568 , 0.49433624, 0.50044235, ..., 0.51843884, 0.50104207, 0.50660972], [0.50155734, 0.49433565, 0.50044284, ..., 0.51843939, 0.50104298, 0.50661071], [0.50155634, 0.4943358 , 0.50044234, ..., 0.51843954, 0.50104276, 0.50660979]]), 4: array([[0.49676083, 0.4894059 , 0.50475826, ..., 0.49040016, 0.51208435, 0.51251658], [0.49676083, 0.4894059 , 0.50475824, ..., 0.49040017, 0.51208433, 0.51251657], [0.49676082, 0.4894059 , 0.50475823, ..., 0.49040017, 0.51208434, 0.51251657], ..., [0.49676082, 0.4894059 , 0.50475825, ..., 0.49040016, 0.51208435, 0.51251657], [0.49676083, 0.48940588, 0.50475824, ..., 0.49040017, 0.51208433, 0.51251656], [0.49676082, 0.48940592, 0.50475825, ..., 0.49040017, 0.51208435, 0.51251658]])}표준편차가 0.01인 정규분포의 활성화값 분포 0.5 근처에 분포 기울기 소실 문제는 발생하지 않는다. 대부분의 활성화 값이 0.5 근처에 분포해서 뉴런을 여러 개 둔 의미가 상실된다. 표현력을 제한한다.Xavier 초깃값 각 층의 활성화값들을 광범위하게 분포시킬 목적 앞 계층의 노드가 $n$개라면 표준편차가 $\\frac 1 {\\sqrt n}$인 분포 사용 앞 층의 노드수가 많을 수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼짐x = np.random.randn(1000, 100)node_num = 100hidden_layer_size = 5activations = {}for i in range(hidden_layer_size): if i != 0: x = activations[i-1] w = np.random.randn(node_num, node_num) / np.sqrt(node_num) a = np.dot(x, w) z = sigmoid(a) activations[i] = zplt.figure(figsize=(20,5))for i, a in activations.items(): plt.subplot(1, len(activations), i+1) plt.title(str(i+1) + \"-layer\") plt.hist(a.flatten(), 30, range=(0,1))plt.show() 앞의 방식들보다 넓게 분포 층이 깊어질수록 형태가 다소 일그러짐, 이는 시그모이드 함수가 (0, 0.5)에서 대칭인 S 곡선이기 때문이다. tanh를 사용하면 해결 가능def tanh(x): return np.tanh(x)x = np.random.randn(1000, 100)node_num = 100hidden_layer_size = 5activations = {}for i in range(hidden_layer_size): if i != 0: x = activations[i-1] w = np.random.randn(node_num, node_num) / np.sqrt(node_num) a = np.dot(x, w) z = tanh(a) activations[i] = zplt.figure(figsize=(20,5))for i, a in activations.items(): plt.subplot(1, len(activations), i+1) plt.title(str(i+1) + \"-layer\") plt.hist(a.flatten(), 30, range=(-1,1))plt.show()활성화 함수로 tanh를 사용 시그모이드를 사용했을 때보다 깔끔한 종 모양 Xavier 초깃값은 원점에서 대칭인 함수가 바람직하다.6.2.3 ReLU를 사용할 때의 가중치 초깃값 RelU에 적합한 초깃값 이용 권장: He 초깃값 앞 계층의 노드가 $n$개일 때, 표준편차가 $\\sqrt{\\frac 2 n}$인 정규분포 사용 ReLU의 반이 0이라서 더 넓게 분포시키기 위해서 2배의 계수가 필요def relu(x): return np.maximum(0, x)x = np.random.randn(1000, 100)node_num = 100hidden_layer_size = 5activations = {}for i in range(hidden_layer_size): if i != 0: x = activations[i-1] w = np.random.randn(node_num, node_num) * 0.01 a = np.dot(x, w) z = relu(a) activations[i] = zplt.figure(figsize=(20,5))for i, a in activations.items(): plt.subplot(1, len(activations), i+1) plt.title(str(i+1) + \"-layer\") plt.hist(a.flatten(), 30, range=(0,1))plt.show()표준편차가 0.01인 정규분포를 가중치 초깃값으로 사용한 경우 학습이 거의 이뤄지지 않음x = np.random.randn(1000, 100)node_num = 100hidden_layer_size = 5activations = {}for i in range(hidden_layer_size): if i != 0: x = activations[i-1] w = np.random.randn(node_num, node_num) / np.sqrt(node_num) a = np.dot(x, w) z = relu(a) activations[i] = zplt.figure(figsize=(20,5))for i, a in activations.items(): plt.subplot(1, len(activations), i+1) plt.title(str(i+1) + \"-layer\") plt.hist(a.flatten(), 30, range=(0,1))plt.show()Xavier 초깃값을 사용한 경우 활성화 값들의 치우침도 커지고, 기울기 소실 문제도 발생x = np.random.randn(1000, 100)node_num = 100hidden_layer_size = 5activations = {}for i in range(hidden_layer_size): if i != 0: x = activations[i-1] w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num) a = np.dot(x, w) z = relu(a) activations[i] = zplt.figure(figsize=(20,5))for i, a in activations.items(): plt.subplot(1, len(activations), i+1) plt.title(str(i+1) + \"-layer\") plt.hist(a.flatten(), 30, range=(0,1))plt.show()He 초깃값을 사용한 경우 모든 층에서 균일하게 분포결론 ReLU: He 초깃값 S자 활성화 함수: Xavier 초깃값6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교# coding: utf-8import osimport syssys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import numpy as npimport matplotlib.pyplot as pltfrom dataset.mnist import load_mnistfrom common.util import smooth_curvefrom common.multi_layer_net import MultiLayerNetfrom common.optimizer import SGD# 0. MNIST 데이터 읽기==========(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)train_size = x_train.shape[0]batch_size = 128max_iterations = 2000# 1. 실험용 설정==========weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}optimizer = SGD(lr=0.01)networks = {}train_loss = {}for key, weight_type in weight_init_types.items(): networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100], output_size=10, weight_init_std=weight_type) train_loss[key] = []# 2. 훈련 시작==========for i in range(max_iterations): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] for key in weight_init_types.keys(): grads = networks[key].gradient(x_batch, t_batch) optimizer.update(networks[key].params, grads) loss = networks[key].loss(x_batch, t_batch) train_loss[key].append(loss) if i % 100 == 0: print(\"===========\" + \"iteration:\" + str(i) + \"===========\") for key in weight_init_types.keys(): loss = networks[key].loss(x_batch, t_batch) print(key + \":\" + str(loss))# 3. 그래프 그리기==========markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}x = np.arange(max_iterations)for key in weight_init_types.keys(): plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)plt.xlabel(\"iterations\")plt.ylabel(\"loss\")plt.ylim(0, 2.5)plt.legend()plt.show()===========iteration:1800===========std=0.01:2.2988839990272347Xavier:0.35191173163622136He:0.31074030545254105===========iteration:1900===========std=0.01:2.3098618922506837Xavier:0.26615511258213476He:0.18684927958842736.3 배치 정규화6.3.1 배치 정규화 알고리즘배치 정규화가 주목받는 이유 학습 속도 개선 초깃값에 크게 의존하지 않는다. 오버피팅 억제배치 정규화(batch normalization) 학습 시 미니배치 단위로 정규화 각 층에서의 활성화값이 적당히 분포되도록 강제한다. 신경망 중간에 배치 정규화 계층을 삽입 수식\\(\\mu_B \\leftarrow \\frac 1 m \\sum^m_{i=1} x_i\\)\\[\\sigma^2_B \\leftarrow \\sum^m_{i=1} (x_i - \\mu_B)^2\\]\\(\\hat {x}_i \\leftarrow \\frac {x_i - \\mu_B} {\\sqrt {\\sigma^2_B + \\epsilon}}\\) 미니 배치의 평균이 0, 분산이 1이 되도록 정규화 $\\epsilon$은 0으로 나누는 것을 방지(10e-7과 같은 매우 작은 값 사용) 활성화 함수 앞이나 뒤에 삽입 배치 정규화 계층마다 정규화된 데이터에 고유한 확대와 이동 변환을 수행한다.\\(y_i \\leftarrow \\gamma \\hat{x}_i + \\beta\\) $\\gamma$: 확대, $\\beta$: 이동▲ 배치 정규화 계산 그래프6.3.2 배치 정규화의 효과MNIST 데이터셋을 사용하여 비교# coding: utf-8import sys, ossys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import numpy as npimport matplotlib.pyplot as pltfrom dataset.mnist import load_mnistfrom common.multi_layer_net_extend import MultiLayerNetExtendfrom common.optimizer import SGD, Adam(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)# 학습 데이터를 줄임x_train = x_train[:1000]t_train = t_train[:1000]max_epochs = 20train_size = x_train.shape[0]batch_size = 100learning_rate = 0.01def __train(weight_init_std): bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, weight_init_std=weight_init_std, use_batchnorm=True) network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, weight_init_std=weight_init_std) optimizer = SGD(lr=learning_rate) train_acc_list = [] bn_train_acc_list = [] iter_per_epoch = max(train_size / batch_size, 1) epoch_cnt = 0 for i in range(1000000000): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] for _network in (bn_network, network): grads = _network.gradient(x_batch, t_batch) optimizer.update(_network.params, grads) if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) bn_train_acc = bn_network.accuracy(x_train, t_train) train_acc_list.append(train_acc) bn_train_acc_list.append(bn_train_acc) print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc)) epoch_cnt += 1 if epoch_cnt &gt;= max_epochs: break return train_acc_list, bn_train_acc_list# 그래프 그리기==========weight_scale_list = np.logspace(0, -4, num=16)x = np.arange(max_epochs)plt.figure(figsize=(20,20))for i, w in enumerate(weight_scale_list): print( \"============== \" + str(i+1) + \"/16\" + \" ==============\") train_acc_list, bn_train_acc_list = __train(w) plt.subplot(4,4,i+1) plt.title(\"W:\" + str(w)) if i == 15: plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2) plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2) else: plt.plot(x, bn_train_acc_list, markevery=2) plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2) plt.ylim(0, 1.0) if i % 4: plt.yticks([]) else: plt.ylabel(\"accuracy\") if i &lt; 12: plt.xticks([]) else: plt.xlabel(\"epochs\") plt.legend(loc='lower right') plt.show()============== 1/16 ==============epoch:0 | 0.093 - 0.11epoch:1 | 0.097 - 0.093C:\\Users\\KJK\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce return ufunc.reduce(obj, axis, dtype, out, **passkwargs)epoch:10 | 0.116 - 0.41epoch:11 | 0.116 - 0.414epoch:12 | 0.116 - 0.421epoch:13 | 0.116 - 0.421epoch:14 | 0.116 - 0.42epoch:15 | 0.116 - 0.421epoch:16 | 0.116 - 0.506epoch:17 | 0.116 - 0.509epoch:18 | 0.116 - 0.512epoch:19 | 0.116 - 0.511 실선이 배치 정규화를 사용한 경우, 점선이 사용하지 않은 경우 거의 모든 경우에서 배치 정규화를 사용한 것이 학습 진도가 빠름6.4 바른 학습을 위해6.4.1 오버피팅 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태 발생 원인 매개변수가 많고 표현력이 높은 모델 훈련 데이터가 적음 MNIST로 실험 300개의 훈련 데이터 7층 네트워크 각층의 뉴런은 100개 ReLU 활성화 함수(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임x_train = x_train[:300]t_train = t_train[:300]network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100, 100], output_size=10)optimizer = SGD(lr=0.01)max_epochs = 201train_size = x_train.shape[0]batch_size = 100train_loss_list = []train_acc_list = []test_acc_list = []iter_per_epoch = max(train_size / batch_size, 1)epoch_cnt = 0for i in range(1000000000): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] grads = network.gradient(x_batch, t_batch) optimizer.update(network.params, grads) if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) epoch_cnt += 1 if epoch_cnt &gt;= max_epochs: breakplt.plot(range(0, 201), train_acc_list, marker=\"o\", markevery=10)plt.plot(range(0, 201), test_acc_list, marker=\"s\", markevery=10)plt.xlabel(\"epochs\")plt.ylabel(\"accuracy\")plt.legend((\"train\", \"test\"), loc=\"lower right\")plt.show()6.4.2 가중치 감소(weigth decay) 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 오버피팅 억제 손실함수에 L2 노름을 더해서 가중치가 커지는 것을 억제 L2 이외에 L1, L$\\infty$ 노름을 정규화 항으로 사용할 수 있다. 손실함수에 $\\frac 1 2 \\lambda \\mathbf{W}^2$를 더한다. $\\lambda$: 정규화 세기 조절하는 하이퍼파라미터 $\\frac 1 2$: 미분 결과인 $\\lambda \\mathbf{W}$을 조정하는 역할의 상수 # coding: utf-8import osimport syssys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import numpy as npimport matplotlib.pyplot as pltfrom dataset.mnist import load_mnistfrom common.multi_layer_net import MultiLayerNetfrom common.optimizer import SGD(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임x_train = x_train[:300]t_train = t_train[:300]# weight decay（가중치 감쇠） 설정 =======================#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우weight_decay_lambda = 0.1# ====================================================network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10, weight_decay_lambda=weight_decay_lambda)optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신max_epochs = 201train_size = x_train.shape[0]batch_size = 100train_loss_list = []train_acc_list = []test_acc_list = []iter_per_epoch = max(train_size / batch_size, 1)epoch_cnt = 0for i in range(1000000000): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] grads = network.gradient(x_batch, t_batch) optimizer.update(network.params, grads) if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc)) epoch_cnt += 1 if epoch_cnt &gt;= max_epochs: break# 그래프 그리기==========markers = {'train': 'o', 'test': 's'}x = np.arange(max_epochs)plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)plt.xlabel(\"epochs\")plt.ylabel(\"accuracy\")plt.ylim(0, 1.0)plt.legend(loc='lower right')plt.show()epoch:195, train acc:0.9266666666666666, test acc:0.7279epoch:196, train acc:0.9366666666666666, test acc:0.7276epoch:197, train acc:0.9333333333333333, test acc:0.7267epoch:198, train acc:0.9333333333333333, test acc:0.7261epoch:199, train acc:0.93, test acc:0.7252epoch:200, train acc:0.9333333333333333, test acc:0.7246여전히 차이는 있지만 이전과 비교하면 차이가 줄어들었다.6.4.3 드롭아웃(dropout) 은닉층의 뉴런을 임의로 삭제하면서 학습하는 방법 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력class Dropout: def __init__(self, dropout_ratio=0.5): self.dropout_ratio = dropout_ratio self.mask = None def forward(self, x, train_flg=True): if train_flg: self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio return x * self.mask else: return x * (1.0 - self.dropout_ratio) def backward(self, dout): return dout * self,mask self.mask: 삭제할 뉴런을 False로 표시 순전파 때 통과시키는 뉴런은 역전파 때도 신호를 그대로 통과시키고, 순전파 때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단# coding: utf-8import osimport syssys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import numpy as npimport matplotlib.pyplot as pltfrom dataset.mnist import load_mnistfrom common.multi_layer_net_extend import MultiLayerNetExtendfrom common.trainer import Trainer(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임x_train = x_train[:300]t_train = t_train[:300]# 드롭아웃 사용 유무와 비울 설정 ========================use_dropout = True # 드롭아웃을 쓰지 않을 때는 Falsedropout_ratio = 0.2# ====================================================network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)trainer = Trainer(network, x_train, t_train, x_test, t_test, epochs=301, mini_batch_size=100, optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)trainer.train()train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list# 그래프 그리기==========markers = {'train': 'o', 'test': 's'}x = np.arange(len(train_acc_list))plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)plt.xlabel(\"epochs\")plt.ylabel(\"accuracy\")plt.ylim(0, 1.0)plt.legend(loc='lower right')plt.show()=== epoch:301, train acc:0.69, test acc:0.5264 ===train loss:1.0455480803766772train loss:1.140440368675071=============== Final Test Accuracy ===============test acc:0.5259 앙상블 학습: 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식 드롭아웃으로 앙상블과 유사한 효과를 낼 수 있다.6.5 적절한 하이퍼파라미터 값 찾기하이퍼파라미터 각 층의 뉴런 수 배치 크기 학습률 가중치 감소 등6.5.1 검증 데이터 하이퍼파라미터의 성능을 평가할 때는 시험 데이터를 사용해선 안 된다. 이유: 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문 검증 데이터(validation data): 훈련 데이터에서 분리해서 하이퍼파라미터의 성능 평가# coding: utf-8import osimport syssys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import numpy as npfrom common.util import shuffle_dataset(x_train, t_train), (x_test, t_test) = load_mnist()# 훈련 데이터를 뒤섞는다.x_train, t_train = shuffle_dataset(x_train, t_train)# 20%를 검증 데이터로 분할validation_rate = 0.20validation_num = int(x_train.shape[0] * validation_rate)x_val = x_train[:validation_num]t_val = t_train[:validation_num]x_train = x_train[validation_num:]t_train = t_train[validation_num:] 데이터 분리 전에 입력 데이터와 정답 레이블을 뒤섞어야 한다. 데이터셋 안의 데이터가 치우쳐 있을 수도 있기 때문6.5.2 하이퍼파라미터 최적화 그리드 서치보다 랜덤 서치가 더욱 좋은 결과를 낸다. 대략적인 범위를 설정하고 점차 줄여나간다. 로그 스케일로 지정과정 0단계하이퍼파라미터 값의 범위를 설정한다. 1단계설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출한다. 2단계1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정확도를 평가한다(에폭은 작게) 3단계1단계와 2단계를 특정 횟수 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다.베이즈 최적화(Bayesian optimization)를 이용해서 효율적 최적화를 수행할 수 있다.6.5.3 하이퍼파라미터 최적화 구현하기학습률과 가중치 감소 계수 탐색하는 문제 가중치 감소 계수: $10^{-8}$~$10^{-4}$ 학습률: $10^{-6}$~$10^{-2}$# coding: utf-8import sys, ossys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정import numpy as npimport matplotlib.pyplot as pltfrom dataset.mnist import load_mnistfrom common.multi_layer_net import MultiLayerNetfrom common.util import shuffle_datasetfrom common.trainer import Trainer(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임x_train = x_train[:500]t_train = t_train[:500]# 20%를 검증 데이터로 분할validation_rate = 0.20validation_num = int(x_train.shape[0] * validation_rate)x_train, t_train = shuffle_dataset(x_train, t_train)x_val = x_train[:validation_num]t_val = t_train[:validation_num]x_train = x_train[validation_num:]t_train = t_train[validation_num:]def __train(lr, weight_decay, epocs=50): network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10, weight_decay_lambda=weight_decay) trainer = Trainer(network, x_train, t_train, x_val, t_val, epochs=epocs, mini_batch_size=100, optimizer='sgd', optimizer_param={'lr': lr}, verbose=False) trainer.train() return trainer.test_acc_list, trainer.train_acc_list# 하이퍼파라미터 무작위 탐색======================================optimization_trial = 100results_val = {}results_train = {}for _ in range(optimization_trial): # 탐색한 하이퍼파라미터의 범위 지정=============== weight_decay = 10 ** np.random.uniform(-8, -4) lr = 10 ** np.random.uniform(-6, -2) # ================================================ val_acc_list, train_acc_list = __train(lr, weight_decay) print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)) key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay) results_val[key] = val_acc_list results_train[key] = train_acc_list# 그래프 그리기========================================================print(\"=========== Hyper-Parameter Optimization Result ===========\")graph_draw_num = 20col_num = 5row_num = int(np.ceil(graph_draw_num / col_num))i = 0plt.figure(figsize=(20,20))for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True): print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key) plt.subplot(row_num, col_num, i+1) plt.title(\"Best-\" + str(i+1)) plt.ylim(0.0, 1.0) if i % 5: plt.yticks([]) plt.xticks([]) x = np.arange(len(val_acc_list)) plt.plot(x, val_acc_list) plt.plot(x, results_train[key], \"--\") i += 1 if i &gt;= graph_draw_num: breakplt.show()=========== Hyper-Parameter Optimization Result ===========Best-1(val acc:0.78) | lr:0.0077307054058675445, weight decay:2.5887086671429946e-05Best-2(val acc:0.77) | lr:0.009154370132240392, weight decay:8.379491394743663e-05Best-3(val acc:0.76) | lr:0.0098006673797611, weight decay:1.0450027144308723e-07Best-4(val acc:0.72) | lr:0.007678681706710489, weight decay:1.7836457077216995e-07Best-5(val acc:0.71) | lr:0.007626052918778914, weight decay:4.5307674366432563e-07Best-6(val acc:0.65) | lr:0.004922531472247901, weight decay:1.7303188009642406e-07Best-7(val acc:0.6) | lr:0.004816515317994456, weight decay:4.499890310553459e-07Best-8(val acc:0.58) | lr:0.003990354694316797, weight decay:1.8853665087085003e-06Best-9(val acc:0.55) | lr:0.0035304311414633004, weight decay:2.637284630879835e-07Best-10(val acc:0.49) | lr:0.0030944498698391794, weight decay:8.008827389280176e-08Best-11(val acc:0.48) | lr:0.0033599550676127856, weight decay:5.599030526130115e-07Best-12(val acc:0.47) | lr:0.0046485659124741955, weight decay:4.803374416952201e-07Best-13(val acc:0.44) | lr:0.005146752703310548, weight decay:1.921828499791707e-06Best-14(val acc:0.43) | lr:0.0019810387426700033, weight decay:1.018064467263664e-07Best-15(val acc:0.36) | lr:0.0031568644707760337, weight decay:9.852039867390541e-05Best-16(val acc:0.34) | lr:0.001842499123551303, weight decay:1.476367368713895e-07Best-17(val acc:0.32) | lr:0.0026945617175910246, weight decay:5.686409658051806e-05Best-18(val acc:0.32) | lr:0.0024649675135479747, weight decay:6.70400523881945e-05Best-19(val acc:0.24) | lr:0.002015741746864137, weight decay:9.886831062118664e-06Best-20(val acc:0.23) | lr:0.0016390246367057597, weight decay:1.3720649280599984e-05학습률은 $0.001$ ~ $0.01$, 가중치 감소 계수는 $10^{-8}$ ~ $10^{-6}$ 사이에서 학습이 잘 진행된다." }, { "title": "밑바닥부터 시작하는 딥러닝 Chapter05", "url": "/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter05/", "categories": "ML", "tags": "Deep_learning", "date": "2021-11-25 00:00:00 +0900", "snippet": "수치 미분은 시간이 오래 걸리는 단점이 있다. 오차역전파법(backpropagation)은 효율적 계산이 가능하다. 수식을 통한 이해 계산 그래프를 통한 이해 ★참고 http://karpathy.github.io/ Stanford CS231n5.1 계산 그래프계산 그래프(computational graph)는 계산 과정을 그래프로 나타낸 것이다. 그래프는 노드(node)와 에지(edge)로 표현된다.5.1.1 계산 그래프로 풀다▲ 간단한 계산 그래프계산 그래프 문제 흐름 계산 그래프를 구성한다. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.계산을 왼쪽에서 오른쪽으로 진행하는 단계를 순전파(forward propagation)이라고 하고 반대 방향을 역전파(backward propagation)이라고 한다.5.1.2 국소적 계산계산 그래프는 국소적 계산을 전파해서 최종 결과를 얻을 수 있다는 특징이 있다. 즉, 다른 부분은 상관하지 않고 자신과 관계된 정보만 출력할 수 있다. 이러한 특징에 따라 각 노드는 자신과 관계된 계산에만 집중하면 된다.▲ 국소적 계산의 예5.1.3 왜 계산 그래프로 푸는가?계산 그래프의 이점 국소적 계산으로 복잡한 문제를 단순화할 수 있다. 역전파를 통해 (다수의) 미분을 효율적으로 계산할 수 있다.▲ 역전파를 통한 미분5.2 연쇄법칙(chain rule)5.2.1 계산 그래프의 역전파국소적 미분은 상류에서 전달된 값과 곱해져서 앞쪽 노드로 전달된다.5.2.2 연쇄법칙이란?연쇄법칙은 합성 함수의 미분이 각 구성 함수의 미분의 곱으로 나타낸다는 성질을 이용한다.예를 들어, $z = (x + y)^2$가 있을때 $x$에 대한 $z$의 미분은 다음과 같이 나타낼 수 있다.\\[\\frac {\\partial z} {\\partial x} = \\frac {\\partial z} {\\partial t} \\frac {\\partial t} {\\partial x}\\]\\[\\frac {\\partial z} {\\partial t} = 2t\\]\\[\\frac {\\partial t} {\\partial x} = 1\\]\\[\\frac {\\partial z} {\\partial x} = \\frac {\\partial z} {\\partial t} \\frac {\\partial t} {\\partial x} = 2t \\cdot 1 = 2(x + y)\\]5.2.3 연쇄법칙과 계산 그래프5.3 역전파5.3.1 덧셈 노드의 역전파덧셈 노드의 역전파는 입력된 값을 그대로 다음 노드로 보낸다.5.3.2 곱셈 노드의 역전파곱셈 노드의 역전파는 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보낸다. 그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장한다.5.3.3 사과 쇼핑의 예▲ 사과 쇼핑의 역전파 예▲ 사과와 귤 쇼핑의 역전파 예5.4 단순한 계층 구현하기5.4.1 곱셈 계층class MulLayer: def __init__(self): self.x = None self.y = None def forward(self, x, y): self.x = x self.y = y out = x * y return out def backward(self, dout): dx = dout * self.y dy = dout * self.x return dx, dy 사과 쇼핑 구현apple = 100apple_num = 2tax = 1.1# 계층들mul_apple_layer = MulLayer()mul_tax_layer = MulLayer()# 순전파apple_price = mul_apple_layer.forward(apple, apple_num)price = mul_tax_layer.forward(apple_price, tax)print(price) # 다들 오차 나는지?220.00000000000003# 역전파dprice = 1dapple_price, dtax = mul_tax_layer.backward(dprice)dapple, dapple_num = mul_apple_layer.backward(dapple_price)print(dapple, dapple_num, dtax)2.2 110.00000000000001 2005.4.2 덧셈 계층class AddLayer: def __init__(self): pass def forward(self, x, y): out = x + y return out def backward(self, dout): dx = dout * 1 dy = dout * 1 return dx, dy덧셈 계층은 그저 상류에서 내려온 미분을 하류로 흘러보내기만 하면 되기 때문에 따로 초기화할 필요가 없다. 사과 2개와 귤 3개를 사는 상황apple_num = 2apple = 100mandarin_num = 3mandarin = 150tax = 1.1mul_apple_layer = MulLayer()mul_mandarin_layer = MulLayer()add_fruit_layer = AddLayer()mul_tax_layer = MulLayer()apple_price = mul_apple_layer.forward(apple, apple_num)mandarin_price = mul_mandarin_layer.forward(mandarin, mandarin_num)fruit_price = add_fruit_layer.forward(apple_price, mandarin_price)total_price = mul_tax_layer.forward(fruit_price, tax)print(total_price)dtotal_price = 1dfruit_price, dtax = mul_tax_layer.backward(dprice)dapple_price, dmandarin_price = add_fruit_layer.backward(dfruit_price)dapple, dapple_num = mul_apple_layer.backward(dapple_price)dmandarin, dmandarin_num = mul_mandarin_layer.backward(dmandarin_price)print(dapple, dapple_num, dmandarin, dmandarin_num)715.00000000000012.2 110.00000000000001 3.3000000000000003 165.05.5 활성화 함수 계층 구현하기5.5.1 ReLU 계층 ReLU 수식\\[y = \\begin{cases}x \\ (x &gt; 0) \\\\0 \\ (x \\leq 0)\\end{cases}\\] ReLU 미분\\[\\frac {\\partial y}{\\partial x} = \\begin{cases}1 \\ (x &gt; 0) \\\\0 \\ (x \\leq 0)\\end{cases}\\]▲ ReLU 계산 그래프import numpy as npclass Relu: def __init__(self): self,mask = None # 입력 원소가 0 이하인 인덱스는 True, 0보다 큰 경우 False 유지 def forward(self, x): self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return 0 def backward(self, dout): dout[self.mask] = 0 dx = dout return dxx = np.array([[1.0, -0.5], [-2.0, 3.0]])print(x)[[ 1. -0.5] [-2. 3. ]]mask = (x &lt;= 0)print(mask)[[False True] [ True False]]mask 인스턴스 변수를 써서 mask의 원소가 True인 곳은 상류에서 전파된 미분값을 0으로 바꾼다.5.5.2 Sigmoid 계층 시그모이드 수식\\[y = \\frac 1 {1+exp(-x)}\\] ’/’ 노드, $y = \\frac 1 x$ 미분\\[\\begin{align}\\frac {\\partial y} {\\partial x} &amp; = -\\frac 1 {x^2} \\\\&amp; = -y^2 \\\\\\end{align}\\] exp 노드 미분\\[\\frac {\\partial y} {\\partial x} = exp(x)\\]▲ 시그모이드 순전파/역전파 sigmoid 미분\\[\\begin{align}\\frac {\\partial y} {\\partial x} &amp; = y^2exp(-x) \\\\&amp; = \\frac 1 {(1 + exp(-x))^2} exp(-x) \\\\&amp; = \\frac 1 {1 + exp(-x)} \\frac {exp(-x)} {1+exp(-x)} \\\\&amp; = y(1-y)\\end{align}\\]시그모이드 계층의 역전파는 순전파의 출력만으로 계산할 수 있다.class Sigmoid: def __init__(self): self.out = None def forward(self, x): out = 1 / (1 + np.exp(-x)) self.out = out return out def backward(self, dout): dx = dout * (1.0 - self.out) * self.out return dx구현에서 순전파의 출력을 out 인스턴스 변수에 저장해 놓고 역전파 계산할 때 사용한다.5.6 Affine/Softmax 계층 구현하기5.6.1 Affine 계층행렬의 곱을 기하학에서는 어파인 변환(affine transformation)이라고 한다.\\[\\frac {\\partial L} {\\partial X} = \\frac {\\partial L} {\\partial Y} \\cdot W^T\\]\\[\\frac {\\partial L} {\\partial W} = X^T \\cdot \\frac {\\partial L} {\\partial Y}\\]계산 그래프에서 각 원소의 형상에 주의해야 한다.5.6.2 배치용 Affine 계층편향의 경우에는 순전파에서 각각의 데이터에 더해진다. 그래서 역전파 때는 편향의 원소에 역전파 값이 편향에 모여야 한다.class Affine: def __init__(self, W, b): self.W = W self.b = b self.x = None self.dW = None self.db = None def forward(self, x): self.x = x out = np.dot(x, self.W) + self.b return out def backward(self, dout): dx = np.dot(dout, self.W.T) self.dW = np.dot(self.x.T, dout) self.db = np.sum(dout, axis=0) return dx5.6.3 Softmax-with-Loss 계층Softmax 계층은 출력의 합이 1이 되도록 정규화하여 출력한다.▲ Softmax-with-Loss 계층의 계산 그래프▲ Softmax-with-Loss 계층 계산 그래프 간소화Softmax 계층의 역전파 결과에서 중요한 점은 Softmax 계층의 출력과 정답 레이블의 차이로, 신경망의 현재 출력과 정답 레이블의 오차를 그래로 드러낸다는 것이다.참고로 항등 함수의 손실 함수로 평균 제곱 오차를 사용하는데 이 때의 역전파 값도 위와 동일하다.# 소프트맥스 오버플로 개선 버전def softmax(a): c = np.max(a) exp_a = np.exp(a - c) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y# 데이터가 1개나 그 이상의 배치로 주어지는 경우def cross_entropy_error(y, t): # y가 1차원, 즉 하나의 데이터일 경우 shape을 바꿔준다. if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum(t * np.log(y + 1e-7)) / batch_sizeclass SoftmaxWithLoss: def __init__(self): self.loss = None # 손실 self.y = None # softmax 출력 self.t = None # 정답 레이블(원-핫 벡터) def forward(self, x, t): self.t = t self.y = softmax(x) self.loss = cross_entropy_error(self.y, self.t) return self.loss def backward(self, dout=1): batch_size = self.t.shape[0] dx = (self.y - self.t) / batch_size return dx5.7 오차역전파법 구현하기5.7.1 신경망 학습의 전체 그림 (생략)5.7.2 오차역전파법을 적용한 신경망 구현하기import sys, ossys.path.append(os.pardir)import numpy as npfrom common.layers import *from common.gradient import numerical_gradientfrom collections import OrderedDictclass TwoLayerNet: def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # 가중치 초기화 self.params = {} self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) # 계층 생성 self.layers = OrderedDict() self.layers['Affine1'] = Affine(self.params[\"W1\"], self.params['b1']) self.layers['Relu1'] = Relu() self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2']) self.lastLayer = SoftmaxWithLoss() def predict(self, x): for layer in self.layers.values(): x = layer.forward(x) return x # x: 입력 데이터, t: 정답 레이블 def loss(self, x, t): y = self.predict(x) return self.lastLayer.forward(y, t) def accuracy(self, x, t): y = self.predict(x) y = np.argmax(y, axis=1) if t.ndim != 1 : t = np.argmax(t, axis=1) accuracy = np.sum(y == t) / float(x.shape[0]) return accuracy # x: 입력 데이터, t: 정답 레이블 def numerical_gradient(self, x, t): loss_W = lambda W: self.loss(x, t) grads = {} grads['W1'] = numerical_gradient(loss_W, self.params['W1']) grads['b1'] = numerical_gradient(loss_W, self.params['b1']) grads['W2'] = numerical_gradient(loss_W, self.params['W2']) grads['b2'] = numerical_gradient(loss_W, self.params['b2']) return grads def gradient(self, x, t): # 순전파 self.loss(x, t) # 역전파 dout = 1 dout = self.lastLayer.backward(dout) layers = list(self.layers.values()) layers.reverse() for layer in layers: dout = layer.backward(dout) # 결과 저장 grads = {} grads['W1'] = self.layers['Affine1'].dW grads['b1'] = self.layers['Affine1'].db grads['W2'] = self.layers['Affine2'].dW grads['b2'] = self.layers['Affine2'].db return grads5.7.3 오차역전파법으로 구한 기울기 검증하기기울기 구하는 방법 수치 미분: 구현은 간단하지만 느리다 해석적 방법: 오차역전파법 이용해서 매개변수 많아도 빠르게 계산 가능실제 학습을 할 땐 계산이 빠른 오차역전파법을 이용하고 수치 미분은 오차역전파법을 정확하게 구현했는지 확인하는 용도로 사용한다. 두 방식으로 기울기가 일치하는 것을 확인하는 작업을 기울기 확인(gradient check)라고 한다.import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)x_batch = x_train[:3]t_batch = t_train[:3]grad_numerical = network.numerical_gradient(x_batch, t_batch)grad_backprop = network.gradient(x_batch, t_batch)# 가 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.for key in grad_numerical.keys(): diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key])) print(key + \": \" + str(diff))W1: 2.0655630540316686e-10b1: 1.1163263739284333e-09W2: 7.232095981457576e-08b2: 1.4492399771914855e-075.7.4 오차역전파법을 사용한 학습 구현하기import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)iters_num = 10000train_size = x_train.shape[0]batch_size = 100learning_rate = 0.1train_loss_list = []train_acc_list = []test_acc_list = []iter_per_epoch = max(train_size / batch_size, 1)for i in range(iters_num): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 오차역전파법으로 기울기를 구한다. grad = network.gradient(x_batch, t_batch) # 갱신 for key in ('W1', 'b1', 'W2', 'b2'): network.params[key] -= learning_rate * grad[key] loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) print(train_acc, test_acc)0.11236666666666667 0.11350.7885 0.79570.8772 0.88080.8986666666666666 0.90360.9084666666666666 0.91270.9139166666666667 0.91730.9194333333333333 0.92130.9230166666666667 0.92520.9273333333333333 0.92940.9311333333333334 0.93120.9345166666666667 0.93360.9368833333333333 0.93720.9398333333333333 0.93860.94185 0.94040.9450833333333334 0.94240.9458666666666666 0.9440.9469 0.9467" }, { "title": "밑바닥부터 시작하는 딥러닝 Chapter04", "url": "/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter04/", "categories": "ML, book_study", "tags": "Deep_learning", "date": "2021-11-20 00:00:00 +0900", "snippet": "CH04 신경망 학습학습이란? 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 의미함4.1 데이터로부터 학습한다=&gt; 신경망의 주요한 특징 中 하나는 데이터를 통해 가중치가 결정된다!퍼셉트론 수렴 정리내용: 선형 분리 문제는 유한번의 학습을 통해 풀 수 있다!정리$ X^+,X^- 가\\ 선형\\ 분리\\ 가능한\\ train-set이라고\\ 하자\\ $$ 이\\ 때\\ y를\\ 다음과\\ 같은\\ 레이블이라\\ 할\\ 때 \\ y_i = \\pm1(x_i \\in X^\\pm)$$ 전체\\ train-set\\ X = X^+ \\cup X^-의$$ N개의\\ 데이터셋에\\ 대해\\ 다음과\\ 같이\\ 순서를\\ 입력한다고\\ 하자$$ x(1),x(2), …..x(N),x(1),…..$$ 즉,\\ 마지막\\ 데이터까지\\ 학습이\\ 끝나면\\ 처음으로\\ 다시\\ 돌아가는\\ 것을\\ 의미함$$ 정리 예측에\\ 실패한\\ 입력값을\\ 다음과\\ 같이\\ 둔다고\\ 할\\ 때\\ x_1,x_2,x_3….x_n$$ w_1=0 즉\\ 가중치\\ 초기값을\\ 0이라\\ 할\\ 때\\ $$ n번째\\ 예측\\ 실패한\\ 데이터에\\ 대해\\ 다음과\\ 같이\\ 가중치를\\ 업데이트\\ 한다고\\ 하면$$ w_{n+1} = w_n + y_nx_n$$ 그러면\\ 다음\\ 식을\\ 만족하는\\ n_0가\\ 존재한다.$$w_{n0} = w_{n0+1}=w_{n0+2} …..$참고 링크: https://freshrimpsushi.github.io/posts/perceptron-convergence-theorem/4.1.1 데이터 주도 학습기계학습 =&gt;데이터에서 답을 찾고 데이터에서 패턴을 발견함 즉, 데이터가 핵심이라고 할 수 있음신경망과 딥러닝은 기존의 기계학습에서 더 나아가 사람의 개입을 최소화해주는 기법이라고 할 수 있음다음과 같은 이미지의 분리 =&gt; 사람의 경우 손쉽게 가능 하지만 로짓을 짜는 것은 다른 문제임기계학습에서 모아진 데이터로부터 규칙을 찾아내는 역할을 기계 즉 컴퓨터가 담당한다.기존의 머신러닝 기법에서는 모델링은 사람이 설계하는 반면에 신경망 모형은 기계가 스스로 학습을 하게 된다.따라서 딥러닝을 종단간 기계학습 (사람의 개입이 기존에 비해 최소화 되었다라는 의미를 내포함)이라고 한다.4.1.2 train-set과 test-set기계 학습에서는 데이터를 train-set과 test-set으로 나누게 된다.있는 데이터를 전부 다 활용하는 것은 왜 안될까? =&gt; 일반화의 문제즉 주어진 데이터에서만 학습을 잘하고 그 이외의 데이터에 대해서는 모델의 성능이 떨어지는 over-fitting 문제가 발생함over-fitting은 대게 변수의 개수가 많을 때(차원의 저주라고 함) 발생하게 된다.4.2 손실 함수(Loss-function)손실 함수 =&gt; 현제 모델의 성능이 어떠한지 나타내는 지표 (정확히는 얼마나 나쁜지를 나타내는 지표)가장 많이 사용되는 손실 함수로는 평균제곱오차(MSE)가 있음\\[E = {1 \\over 2} \\sum_{k}(y_k - t_k)^2\\]\\[이\\ 때\\ y_k는\\ 신경망\\ 모형의\\ 추정값\\ t_k는\\ 정답\\ 레이블을\\ 의미함\\]평균 제곱오차를 python으로 구현하는 코드는 다음과 같다import numpy as npy = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.0]t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]# 평균제곱 오차의 구현def mean_squared_error(y, t): return 0.5 * np.sum((y-t)**2)# 실제 사용# 실제 정답은 2인 상황t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]# 예시: 2일 확률이 가장 높다고 추정함y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]mean_squared_error(np.array(y),np.array(t)) #y와t가 리스트 형태로 저장이 되어 있으므로 넘파이 배열로 변환 후 계산0.09750000000000003# 7일 확률이 가장 높다고 추정한 경우y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]mean_squared_error(np.array(y),np.array(t))# 위의 예시로부터 정답을 맞추면 평균제곱오차가 줄어드는 것을 확인할 수 있음0.59754.2.2 cross-entropy신경망 모형에서 자주 사용되는 또 다른 손실 함수로 cross-entropy가 있음\\[E =- \\sum_{k}t_k\\log y_k\\]이 때 log는 일반적으로 자연상수 \\(e\\)를 의미함 즉 자연로그위 그래프에서 확인이 가능하듯히 자연로그 함수는 0에 가까울수록 y의 값이 작아짐 (손실이 작아짐)다음은 교차 엔트로피의 python 구현 코드\\[이\\ 때\\ 아주\\ 작은\\ 임의의\\ 수를\\ 더해주어\\ -\\infty 가\\ 되는\\ 것을\\ 방지해\\ 줌\\]# cross entropy 구현def cross_entropy_error(y, t): delta = 1e-7 # 보정항 return -np.sum(t * np.log(y+delta))t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]cross_entropy_error(np.array(y), np.array(t))0.510825457099338y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]cross_entropy_error(np.array(y), np.array(t))2.3025840929945464.2.3 미니배치 학습모든 훈련 데이터의 손실 함수 공식은 다음과 같다.\\[E= -{1 \\over N}\\sum_{n}\\sum_{k}t_{n,k} \\log y_{n,k}\\]이는 단일 데이터의 손실 함수 공식을 전체 데이터의 개수 N개로 확장한 것이고, 마지막으로 데이터의 개수 N개로 나눠서정규화된 손실함수를 구할 수 있다하지만 일반적으로 모든 데이터의 손실함수를 구하는 것은 굉장한 computation 낭비가 발생하게 된다.(예시에서는 60,000개의 데이터가 존재하고 일반적으로 빅데이터로 확장하게 되었을 때 계산량이 어마어마 해짐)따라서 신경망 모형에서는 손실함수를 계산할 때 모든 훈련 데이터에 대해서 손실 함수를 구하지 않고일부를 추출하여 (일종의 random-sampling 기법) 그 일부에 대해서만 손실 함수를 구하게 된다.다음은 mnist 데이터에서 일부를 무작위로 골라내는 코드를 구현한 것임import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnist(x_train, y_train), (x_test, y_test) = load_mnist(normalize = True, one_hot_label = True) # one-hot encdoing 방식으로 class를 범주화print(x_train.shape)print(y_train.shape)(60000, 784)(60000, 10)train_size = x_train.shape[0]batch_size = 10batch_mask = np.random.choice(train_size, batch_size) # 무작위 추출x_batch = x_train[batch_mask]t_batch = y_train[batch_mask]np.random.choice(6000, 10)# 딥러닝에서의 배치사이즈는 통계학에서의 샘플링과 동일한 의미를 지닌다고 볼 수 있음array([ 333, 3044, 5502, 1494, 5039, 210, 1476, 2268, 245, 1344])미니 배치 학습은 통계학에서의 sampling 기법을 적용한 것이라고 할 수 있음통계학은 모집단의 정보를 추출하기 위해 모집단에서 표본을 추출하여 모집단의 정보를 추정하는데,만약 적절한 표본 채집 방법과 표본의 개수를 적용한다면 굳이 모든 모집단의 표본을 가지고 있지 않아도올바른 추정이 가능함# 배치용 cross_entropy 구현하기def cross_entropy_error(y, t): if y.dim ==1: t = t.reshape(1, t.size) # y는 신경망에서의 추정값 t는 실제 정답 레이블을 의미하게 된다 y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum(t * np.log(y+1e-7)) / batch_sizedef cross_entropy_error(y, t): if y.dim ==1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum(np.log(y[np.arange(batch_size), t]+1e-7)) / batch_size # 전체 훈련 데이터의 개수로 나눈 것이 아닌 batch_size로 나누게 된다.4.2.5 왜 손실함수를 설정하는가?신경망 모형에서 손실함수의 미분값을 기준으로 가중치를 갱신하는 방법을 사용함 (더 자세한 내용은 뒤에 다뤄질 듯)따라서 정확도를 사용하게 된다면 거의 대부분의 미분 값이 0이 되기 때문에 가중치 갱신이 안된다 즉 훈련이 불가함accuracy를 사용했을 때 미분값이 0이 된다는 것의 의미=&gt; 가중치를 미세하게 조정하더라도 정확도의 값은 그대로 값을 유지하는 경우가 대다수가 될 것임하지만 손실 함수를 선택할 경우 가중치의 미세한 조정에 따라서 손실 함수의 값 역시 지속적으로 변동이 되기 때문에그에 대한 반응이 일어나고 미분으로 계산할 시 0이 아니게 된다. 즉 가중치의 갱신이 일어난다!4.3 수치 미분미분의 의미 =&gt; 어떤 함수가 존재하고 어떤 지점이 있을 때 해당 지점에서의 순간 변화율을 의미함 수식으로는 다음과 같다.\\[\\lim_{h \\to 0}{f(x+h)-f(x) \\over h}\\]\\[즉\\ x의\\ 작은\\ 변화가\\ f(x)를\\ 얼마나\\ 변화시키는가\\ 를\\ 나타낸다.\\]다음은 미분을 python으로 구현한 코드# 나쁜 코드의 예시def numerical_diff(f, x): h = 10e-50 return ((f(x+h)) -f(x))/h # 프로그래밍에서의 반올림 오차를 발생하게 된다.# 반올림의 문제를 어느 정도 해결def numerical_diff(f, x): h = 1e-4 return ((f(x+h)) -f(x))/h결국 python으로 미분을 구현하는 것은 수치 미분을 적용하는 것과 같음why =&gt; 애초에 실제 손실 함수가 어떠한 수식으로 이루어 졌는지 정확하게 알아내는 것이 불가능함그러므로 근사적으로 수치미분을 사용하여 신경망 모형의 가중치를 갱신하는 것이 타당할 것임해석적 미분 =&gt; 실제 미분값을 의미 예를 들어 \\({\\partial \\over \\partial{x}} x^2 = 2x\\)수치 미분 =&gt;아주 작은 차분으로 미분하는 것을 의미 즉 위의 python code로 구현한 것을 의미한다고 보면 된다. (쉽게 말해 해석적 미분의 근사치)def function_1(x): return 0.01*x**2 + 0.1*ximport numpy as npimport matplotlib.pylab as pltx = np.arange(0.0, 20.0, 0.1)y = function_1(x)plt.xlabel(\"X\")plt.ylabel(\"f(X)\")plt.plot(x,y)plt.show()numerical_diff(function_1, 5) # x = 5에서의 수치미분값0.20000099999917254numerical_diff(function_1, 10) # x = 10에서의 수치미분값0.30000099999760724.3.3 편미분일반적으로 우리가 계산하고 미분해야할 손실 함수는 여러 가중치들의 함수로 표현이 된다.이러한 여러 변수에 대한 미분을 적용하기 위한 방법으로 편미분 방법을 적용하는데,함수를 다음과 같이 가정할 때 편미분은 다음과 같다.\\[f(x_0,x_1) = {x_0}^2 + {x_1}^2\\]\\[{\\partial \\over \\partial{x_0}} f(x_0,x_1) = 2x_0\\]\\[{\\partial \\over \\partial{x_1}} f(x_0,x_1) = 2x_1\\]이를 python으로 구현한 코드는 다음과 같다.def function_2(x): return x[0]**2 + x[1]**2def function_tmp1(x0): return x0*x0 + 4.0**2.0numerical_diff(function_tmp1,3.0)6.000099999994291def function_tmp2(x1): return 3.0*3.0 + x1*x1numerical_diff(function_tmp2,4.0)8.00009999998963위의 함수의 그래프는 다음과 같이 그릴 수 있음4.4 기울기기울기란? 여러 변수의 편미분 값의 vectorization 즉 여러 변수의 편미분 값을 벡터로 표현한 것이 기울기라고 할 수 있음def numerical_gradient(f, x): h = 1e-4 grad = np.zeros_like(x) # x와 같은 형상의 배열을 생성 for idx in range(x.size): tmp_val = x[idx] x[idx] = tmp_val + h fxh1 = f(x) x[idx] = tmp_val -h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val return gradnumerical_gradient(function_2, np.array([3.0, 4.0]))array([6., 8.])numerical_gradient(function_2, np.array([0.0, 2.0]))array([0., 4.])numerical_gradient(function_2, np.array([3.0, 0.0]))array([6., 0.])손실 함수를 convex-function으로 가정하게 된다면 해당 함수를 단면으로 자른 모습은 다음과 같다따라서 손실함수를 최솟값을 가지게 하려면 해당 위치에서 각각 편미분을 하여 벡터로 표현한 값에 -를 곱해주면손실함수의 최소가 되는 지점으로 나아갈 수 있다.이 것이 gradient-descent 방법이다.(local_minimum 혹은 실제로 convex 형태가 아닐 가능성이 있는 문제가 있지만 이는 여기서 다루지 않고 후에 다루도록 한다.)경사법을 수식으로 나타내면 다음과 같다.\\(x_0 =x_0 - \\eta{\\partial f \\over \\partial{x_0}}\\)\\(x_1 =x_1 - \\eta{\\partial f \\over \\partial{x_1}}\\)여기서 eta는 학습률(얼마나 나아갈 지를 의미)이다.다음은 경사하강법의 구현def gradient_descent(f, init_x, lr = 0.01, step_num = 100): x = init_x for i in range(step_num): grad = numerical_gradient(f, x) x -= lr*grad return xdef function_2(x): return x[0]**2 + x[1]**2init_x = np.array([-3.0, 4.0])gradient_descent(function_2, init_x, lr = 0.01, step_num = 100)array([-0.39785867, 0.53047822])Note: 위에서와 같이 학습률과 같은 매개변수를 hyperparameter라 하는데 이는 사용자가 지정해야 함optimal한 hyperparameter를 찾는 방식에 대해서는 이후에 포스팅을 할 예정# 학습률이 너무 큰 예init_x = np.array([-3.0, 4.0])gradient_descent(function_2, init_x, lr = 10, step_num = 100)array([-2.58983747e+13, -1.29524862e+12])# 학습률이 너무 작은 예init_x = np.array([-3.0, 4.0])gradient_descent(function_2, init_x, lr = 1e-10, step_num = 100)array([-2.99999994, 3.99999992])4.4.2 신경망에서의 기울기신경망 학습에서의 기울기를 구하는 과정을 간단하게 수식으로 나타내면 다음과 같음 가중치를(2*3)으로 가정하고 수식 전개\\(\\begin{matrix} w_{11} &amp; w_{12} &amp; w_{13} \\\\ w_{21} &amp; w_{22} &amp; w_{23} \\\\\\end{matrix}\\)\\(\\left[\\begin{matrix}{\\partial L \\over \\partial{w_{11}}} &amp; {\\partial L \\over \\partial{w_{12}}} &amp; {\\partial L \\over \\partial{w_{13}}} \\\\{\\partial L \\over \\partial{w_{21}}} &amp; {\\partial L \\over \\partial{w_{22}}} &amp; {\\partial L \\over \\partial{w_{23}}} \\\\\\end{matrix}\\right]\\)이를 python으로 구현하면 다음과 같다. (초기 가중치 설정에 관련한 문제는 여기에서는 다루지 않고 뒤에서 다룰 예정)from common.functions import softmax, cross_entropy_errorfrom common.gradient import numerical_gradientclass simpleNet: def __init__(self): self.W = np.random.randn(2, 3) def predict(self, x): return np.dot(x, self.W) def loss(self, x, t): z = self.predict(x) y=softmax(z) loss = cross_entropy_error(y, t) return lossnet = simpleNet()print(net.W) # 가중치 매개변수x = np.array([0.6, 0.9])p=net.predict(x)print(p)np.argmax(p) # 최댓값을 갖는 인덱스t = np.array([0, 0, 1]) # 정답 레이블net.loss(x, t)[[-2.07245772 -0.56657407 -1.66994278] [ 0.33661394 1.13914085 1.04237842]][-0.94052209 0.68528232 -0.06382508]1.2616562177357782def f(W): return net.loss(x, t) dw = numerical_gradient(f, net.W)print(dw)[[ 0.25810928 0.03812922 -0.2962385 ] [ 0.38716392 0.05719382 -0.44435774]]f = lambda w: net.loss(x, t)dw = numerical_gradient(f, net.W)4.5 학습 알고리즘의 구현경사하강법을 사용하여 가중치를 갱신한다. 이 때 데이터를 미니배치로 무작위로 선정하기 때문에확률적 경사하강법(stocastic gradient descent)라 한다. 보통 SGD로 사용mnist 데이터를 이용한 신경망 모델링from common.functions import *from common.gradient import numerical_gradientclass TwoLayerNet: def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01): # 가중치 초기화 self.params ={} self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) def predict(self, x): W1, W2 = self.params[\"W1\"], self.params[\"W2\"] b1, b2 = self.params['b1'], self.params['b2'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 y = softmax(a2) return y # x 입력 데이터 , t: 정답 레이블 def loss(self, x, t): y= self.predict(x) return cross_entropy_error(y, t) def accuracy(self, x, t): y = self.predict(x) y = np.argmax(y, axis = 1) t = np.argmax(t, axis = 1) accuracy = np.sum(y == t) / float(x.shape[0]) return accuracy def numerical_gradient(self, x, t): loss_W = lambda W: self.loss(x, t) grads = {} grads['W1'] = numerical_gradient(loss_W, self.params[\"W1\"]) grads['b1'] = numerical_gradient(loss_W, self.params[\"b1\"]) grads['W2'] = numerical_gradient(loss_W, self.params[\"W2\"]) grads['b2'] = numerical_gradient(loss_W, self.params[\"b2\"]) return grads net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)net.params['W1'].shapenet.params['b1'].shapenet.params['W2'].shapenet.params['b2'].shape(10,)x = np.random.rand(100, 784)y = net.predict(x)x = np.random.rand(100, 784)t = np.random.rand(100, 10)grads = net.numerical_gradient(x, t)print(grads['W1'].shape,grads['b1'].shape,grads['W2'].shape,grads['b2'].shape)(784, 100) (100,) (100, 10) (10,)미니 배치 학습의 구현import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)train_loss_list = []# 하이퍼파라미터 세팅iters_num = 1000train_size = x_train.shape[0]batch_size = 100learning_rate = 0.1network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)for i in range(iters_num): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] grad = network.numerical_gradient(x_batch, t_batch) for key in (\"W1\",\"b1\",\"W2\",\"b2\"): network.params[key] -= learning_rate*grad[key] loss = network.loss(x_batch, t_batch) train_loss_list.append(loss)plt.xlabel(\"iter_num\")plt.ylabel(\"Loss of model\")plt.plot(iter_num,train_loss_list)plt.show()4.5.3 시험 데이터로 평가하기신경망 학습의 목표는 범용성의 확보! 따라서 훈련에 사용되지 않은 데이터를 통해 해당 모형을 평가하는 것이 중요epoch =&gt; 하나의 단위 훈련 데이터를 미니배치를 통해 학습을 할 경우 해당 미니배치를 다 소진한다면 1epoch가 실행된 것임import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)train_loss_list = []tarin_acc_list = []test_acc_list = []# 하이퍼파라미터 세팅iters_num = 1000train_size = x_train.shape[0]batch_size = 100learning_rate = 0.1# 1에폭당 반복수iter_per_epoch = max(train_size / batch_size, 1)network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)for i in range(iters_num): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] grad = network.numerical_gradient(x_batch, t_batch) for key in (\"W1\",\"b1\",\"W2\",\"b2\"): network.params[key] -= learning_rate*grad[key] loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) if i % iter_per_epoch ==0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) 4.6 정리 기계학습에서는 train-set과 test-set으로 나누게 된다. 훈련 데이터의 범용성을 확인하기 위한 방법으로 test-set을 사용하게 된다. 신경망 모형의 지표로 손실함수를 사용하게 된다. 이 때 손실함수가 작은 것이 좋음 해석적 미분을 구현하기 어려우므로 수치미분을 사용한다. 다음장에서 오차역전파법을 통해 이를 개선할 예정 가중치 매개변수를 갱신하기 위해서 매개변수의 기울기를 이용하게 된다. " }, { "title": "밑바닥부터 시작하는 딥러닝 Chapter03", "url": "/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter03/", "categories": "ML, book_study", "tags": "Deep_learning", "date": "2021-11-14 00:00:00 +0900", "snippet": "신경망가중치 매개변수의 적절한 값을 데이터에서 자동으로 학습하는 능력이 신경망의 중요한 성질이다.신경망의 개요, 신경망이 입력 데이터가 무엇인지 식별하는 처리 과정을 알아보자.퍼셉트론에서 신경망으로퍼셉트론과 다른 점으로 신경망 구조를 살펴보자.신경망의 예신경망 그림을 참고한다.가장 왼쪽 줄을 입력층, 가장 오른쪽 줄을 출력층, 중간 줄을 은닉층이라 한다.은닉층의 뉴런은 사람 눈에 보이지 않는다. 0층이 입력층, 1층이 은닉층, 2층이 출력층이 된다.가중치를 갖는 층이 2개이므로 2층 신경망이라 하며, 문헌에 따라 3층 신경망이라고도 한다.퍼셉트론 복습이 퍼셉트론을 수식으로 나타내면 다음과 같다.\\[y =\\begin{cases} 0 &amp; \\mbox{}(b+w_1x_1+w_2x_2) \\le\\mbox{0}\\\\ 1 &amp; \\mbox{}(b+w_1x_1+w_2x_2)&gt;\\mbox{0}\\end{cases}\\]$b$는 편향을 나타내는 매개변수 : 뉴런이 얼마나 쉽게 활성화되는지 제어$w_1, w_2$ 각 신호의 가중치를 나타내는 매개변수 : 각 신호의 영향력 제어.편향을 명시한 퍼셉트론은 다음과 같다.가중치가 b이고 입력이 1인 뉴런을 추가했다. 그러면 3개의 신호가 뉴런에 입력되고, 신호에 가중치를 곱해서 다음 뉴런에 전달 된다.다음 뉴런은 이 신호의 값을 더해서 총 합이 0을 넘으면 1을 출력하고 아니면 0을 출력한다.편향의 입력 신호는 항상 1이므로 회색으로 구분한다.더 간결하게 작성애보면 다음과 같은 식으로 작성한다. 여기서 조건 분기의 동작을 하나의 함수 $h(x)$로 표현한다.$y = h(b + w_1x_1 + w_2x_2)$\\[h(x) =\\begin{cases}0 &amp; \\mbox{} (x \\le\\mbox{0}) \\\\1 &amp; \\mbox{} (x &gt;\\mbox{0})\\end{cases}\\]결과적으로 퍼셉트론의 식과 위의 식은 동일하다고 볼 수 있다.활성화 함수의 등장 활성화 함수 : $h(x)$처럼 입력 신호의 총합을 출력 신호로 변환하는 함수입력 신호의 총합이 활성화를 일으키는지 정한다.2단계로 다시 분리한다. 입력 신호의 총합을 계산하고, 그 합을 활성화 함수에 입력해 결과를 낸다.\\(a = b+w_1x_1 + w_2x_2\\)입력신호와 편향의 총합을 계산해서 $a$라고 한다.\\(y = h(a)\\) $a$를 함수 $h()$에 넣어 $y$를 출력한다. 활성화 함수의 처리 과정은 다음과 같다. 뉴런과 노드는 같은 의미로 사용한다. 신경망 동작을 명확히 하고자 활성화 과정을 명시했다.$a$ : 입력 신호의 총합$h()$ : 활성화 함수$y$ : 출력활성화 함수 계단 함수(step function) : 임계값을 경계로 출력이 바뀌는 활성화 함수.“퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다.”활성화 함수로 계단 함수에서 다른 함수로 변경하는 것이 신경의 열쇠이다.시그모이드 함수시그모이드 함수(sigmoid function)$h(x) = {1 \\over {1+\\exp(-x)}}$시그모이드는 단순한 함수로 변환기이다. 시그모이드 함수로 신호를 변환하고, 변환된 신호를 다음 뉴런에 전달한다.퍼셉트론과 신경망의 차이가 이 활성화 함수이다.#계단함수 구현하기def step_function(x):#넘파이 배열은 인수로 넣지 못한다. if x &gt;0: return 1 else: return 0#넘파이 배열도 가능하게 하자.def step_function(x): y = x&gt;0 return y.astype(np.int)import numpy as npx = np.array([-1.0, 1.0, 2.0])xarray([-1., 1., 2.])y = x &gt; 0 # 넘파이 배열에 부등호 연산 수행하면, 배열의 원소 각각 부등호 연산에 bool 배열이 나온다. 0 보다 크면 true, 나머지 false 출력한다.yarray([False, True, True])y = y.astype(np.int) # 자동 형변환을 해줄 수 있다. 계단 함수는 0이나 1의 int형을 출력하기 때문이다.y # 좋은 넘파이의 트릭이다. astype() 메서드 활용하자!array([0, 1, 1])# 계단 함수의 그래프import numpy as npimport matplotlib.pylab as pltdef step_function(x): return np.array(x &gt;0, dtype = np.int)x = np.arange(-5.0, 5.0, 0.1) #-5 부터 5까지 0.1 간격으로 넘파이 배열 생성y = step_function(x)plt.plot(x, y)plt.ylim(-0.1, 1.1) # y축 범위plt.show()# 시그모이드 함수 구현하기def sigmoid(x): return 1 / (1 + np.exp(-x)) # 인수 x가 넘파이 배열이라도 올바른 결과 나온다. 기억!x = np.array([-1.0, 1.0, 2.0])sigmoid(x) # 넘파이의 브로드 캐스트 덕분에 넘파이 배열도 가능한 것이다.array([0.26894142, 0.73105858, 0.88079708])t = np.array([1.0, 2.0, 3.0])1.0 + tarray([2., 3., 4.])1.0 / t #스칼라 값과 넘파이 배열이 계산이 가능하다. sigmoid도 가능한 이유!array([1. , 0.5 , 0.33333333])x = np.arange(-5.0, 5.0, 0.1)y = sigmoid(x)plt.plot(x,y)plt.ylim(-0.1, 1.1)plt.show() #시그모이드 = S자 모양만 기억하라!!시그모이드 함수와 계단 함수 비교 매끄러움의 차이시그모이드는 부드러운 곡선이고 입력에 따른 출력이 연속적으로 변화. 계단 함수는 0을 경계로 출력이 갑자기 변화한다. 시그모이드의 매끈함이 신경망에서 아주 중요하다. 출력 값의 차이계단 함수는 0과 1 중 하나의 값만 출력하고, 시그모이드는 실수를 돌려준다. 신경망에서는 연속적인 실수가 흐른다. 공통점큰 관점에서는 둘은 같은 모양을 한다. 입력이 작을 때의 출력이 0에 가깝거나 0이고, 입력이 커지면 출력이 1에 가깝거나 1이 되는 구조이다.입력이 중요하면 큰 값을 출력하고, 입력이 중요하지 않으면 작은 값을 출력한다.입력이 아무리 작거나 커도 0에서 1 사이 출력을 낸다.비선형 함수공통점으로 두 함수모두 비선형 함수이다.선형 함수 : 변환기에 입력 시 출력이 입력의 상수배만 가능.비선형 함수 : 선형이 아닌 함수.신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다.선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어진다.\\(h(x) = cx \\\\y(x) = h(h(h(x))) \\\\y(x) = c * c * c * x \\\\y(x) = ax \\\\a = c^3\\)결국 동일한 선형 함수가 된다. 은닉층 없는 네트워크이다. 층을 쌓는 혜택을 얻으려면 비선형 함수를 반드시 써라! 비선형 함수가 층을 쌓는 의미를 가지는 이유?ReLu 함수(Rectified Linear Unit)최근에는 이 ReLu 함수를 주로 이용한다.입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수로 다음과 같다.\\(h(x) =\\begin{cases}x &amp; \\mbox{} (x &gt;\\mbox{0}) \\\\0 &amp; \\mbox{} (x \\le\\mbox{0})\\end{cases}\\)def relu(x): return np.maximum(0, x) #maximum은 두 입력 값 중 큰 값을 선택해 반환하는 함수이다.x = np.arange(-5.0, 3.0, 0.1)y = relu(x)plt.plot(x,y)plt.ylim(-0.1, 1.1)plt.show() #시그모이드 = S자 모양만 기억하라!!다차원 배열의 계산다차원 배열신경망을 효율적으로 구현한다. 기본은 ‘숫자의 집합’이다. N차원으로 숫자를 나열하는 것을 통틀어 다차원 배열이라 한다.import numpy as np# 1차원 배열A = np.array([1,2,3,4])print(A)[1 2 3 4]np.ndim(A) #넘파이 차원을 확인한다.1A.shape # 배열의 형상을 인스턴스 변수인 shape으로 본다. 배열의 원소 크기 보여준다. 튜플로 변환하는 것을 주의하라!!! 다차원 배열에서 모두 통일된 형태로 반환하기 위함이다.(4,)A.shape[0]4# 2차원 배열B = np.array([[1,2], [3,4], [5,6]])print(B)[[1 2] [3 4] [5 6]]np.ndim(B)2B.shape(3, 2)B는 $3 \\times 2$ 인 배열로,처음 차원은 원소가 3개이고 다음 차원은 원소가 2개라는 의미이다.2차원 배열은 행렬 matrix라고 부르고, 가로 방향을 행 row, 세로 방향을 열 column이라고 한다.$\\pmatrix {1\\ 2 3\\ 45\\ 6}$행렬의 곱A = np.array([[1,2], [3,4]])A.shape(2, 2)B = np.array([[5,6], [7,9]])B.shape(2, 2)np.dot(A, B) #두 행렬의 곱은 넘파이 함수 np.dot()을 이용! 1차원 배열 입력은 벡터, 2차원 배열은 행렬 곱을 계산한다. 교환법칙 성립하지 않는다.array([[19, 24], [43, 54]])# 형상이 다른 경우 행렬의 곱A = np.array([[1,2,3], [4,5,6]])A.shape(2, 3)B = np.array([[1,2],[3,4],[5,6]])B.shape(3, 2)np.dot(A, B)array([[22, 28], [49, 64]])C = np.array([[1,2],[3,4]])C.shape(2, 2)A.shape(2, 3)np.dot(A, C) #A의 1번째 차원과 C의 0번째 차원이 다르면 오류가 난다.(차원의 인덱스는 0부터 시작한다.) ---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-57-f2867cbaa5d7&gt; in &lt;module&gt;----&gt; 1 np.dot(A, C) #A의 1번째 차원과 C의 0번째 차원이 다르면 오류가 난다.(차원의 인덱스는 0부터 시작한다.)ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)대응하는 차원의 원소 수를 일치시켜라!!A = np.array([[1,2],[3,4],[5,6]])A.shape(3, 2)B = np.array([7,8])B.shape(2,)np.dot(A, B)array([23, 53, 83])신경망에서의 행렬 곱넘파이 행렬로 신경망을 구현하자. 편향과 활성화 함수를 생략하고 가중치만 가정한다.주의할 점은 X와 W의 대응하는 차원의 원소 수가 같아야 한다.X = np.array([1,2])X.shape(2,)W = np.array([[1,3,5], [2,4,6]])W.shape(2, 3)y = np.dot(X, W) # np.dot() 으로 아무리 Y 원소가 커도 한 번의 연산으로 계산한다!! 행렬의 곱으로 한꺼번에 계산해주는 기능이 신경망에서 매우 중요하다.print(y)[ 5 11 17]3층 신경망 구현하기넘파이 배열로 신경망의 순방향 처리 완성할 수 있다.표기법 설명$w_{12}^{(1)}$ : 앞 층의 2번째 뉴런($x_2$)에서 다음 층의 1번째 뉴런($a_1^{(1)}$)으로 향할 때 가중치이다.하단 인덱스 번호는 다음 층 번호, 앞 층 번호 순으로 적는다.#행렬의 곱으로 구현하기X = np.array([1.0, 0.5]) # (2,)W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) # (2,3)B1 = np.array([0.1, 0.2, 0.3]) # (3,)A1 = np.dot(X, W1) + B1# 은닉층에서 가중치 합을 a로 표기, 활성화 함수 h() 변환된 신호를 z로 표기# 시그모이드로 표현한다.Z1 = sigmoid(A1) # 앞에서 정의print(A1)print(Z1)[0.3 0.7 1.1][0.57444252 0.66818777 0.75026011]#1층에서 2층으로 가는 과정W2 = np.array([[0.1, 0.4], [0.2, 0.3], [0.3, 0.6]]) #(3,2)B2 = np.array([0.1, 0.2]) #(2,)A2 = np.dot(Z1, W2) # 1*3 * 3*2 = 1*2 (2,)Z2 = sigmoid(A2)print(Z2) #Z1이 2층의 입력이 된다.[0.60256397 0.70690291]def identity_function(x): #항등함수 정의하고, 이를 출력층의 활성화 함수로 이용한다. return xW3 = np.array([[0.1, 0.3], [0.2, 0.4]])B3 = np.array([0.1, 0.2])A3 = np.dot(Z2, W3) + B3Y = identity_function(A3) # Y= A3 출력층의 함수로 그림에서는 시그마 함수로 표기한다.# 구현 정리def init_network(): network = {} network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) # (2,3) network['b1'] = np.array([0.1, 0.2, 0.3]) #(3,) network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) #(3,2) network['b2'] = np.array([0.1, 0.2]) #(2,) network['W3'] = np.array([[0.1,0.3], [0.2, 0.4]]) #(2,2) network['b3'] = np.array([0.1, 0.2]) #(2,) return networkdef forward(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = identity_function(a3) return ynetwork = init_network() # 가중치와 편향을 초기화하고 이를 딕셔너리 변수인 network에 저장한다. 여기에 각 층에 필요한 매개변수 저장한다.x = np.array([1.0, 0.5])y = forward(network, x) #입력신호를 출력으로 변환하는 처리 과정 구현한다. 신호가 순방향으로 전달됨(순전파) 표기한 이름이다.print(y) #[0.31682708 0.69627909]# 넘파이 다차원 배열 쓰면 효율적으로 신경망 구현이 가능하다![0.31682708 0.69627909]출력층 설계하기신경망은 분류와 회귀 모두 이용한다. 활성화 함수가 다르다. 일반적으로 회귀는 항등함수를 쓰고 분류는 소프트맥스함수를 쓴다.항등 함수와 소프트맥스 함수 구현하기 항등 함수(identity function) : 입력을 그대로 출력한다. 입력과 출력이 항상 같다. 소프트맥스 함수(softmax function)\\(y_k = {\\exp(a_k) \\over \\sum_{i=1}^n \\exp(a_i)}\\)n은 출력 층의 뉴런 수이고, $y_k$는 k번째 출력임을 뜻한다. 분자는 입력 신호의 지수 함수이고, 분모는 모든 입력 신호의 지수 함수의 합이다.소프트맥스는 분모에서 보듯이 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받는다.a = np.array([0.3, 2.9, 4.0])exp_a = np.exp(a)print(exp_a)[ 1.34985881 18.17414537 54.59815003]sum_exp_a = np.sum(exp_a)print(sum_exp_a)74.1221542101633y = exp_a / sum_exp_aprint(y)[0.01821127 0.24519181 0.73659691]# 함수로 만든다.def softmax(a): exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y소프트맥스 함수 구현시 주의점오버플로 문제. 지수 함수를 사용해서 쉽게 큰 값을 내므로, 큰 값끼리 나누면 결과가 불안정해진다. 해결하려면 다음과 같이 개선한다.\\(y_k = {\\exp(a_k) \\over \\sum_{i=1}^n \\exp(a_i)} = {C\\exp(a_k) \\over C\\sum_{i=1}^n \\exp(a_i)} \\\\ = {\\exp(a_k + \\log C) \\over \\sum_{i=1}^n \\exp(a_i + \\log C)}\\\\ = {\\exp(a_k + C') \\over \\sum_{i=1}^n \\exp(a_i + C')}\\)$C’$ : 오버플로를 막기 위해서는 입력 신호 중 최댓값을 이용한다.a = np.array([1010, 1000, 900])np.exp(a) / np.sum(np.exp(a)) # 오버 플로 문제가 발생한다. nan = not a number/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp /usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide​array([nan, nan, nan])c = np.max(a)a - carray([ 0, -10, -110])np.exp(a - c) / np.sum(np.exp(a - c))array([9.99954602e-01, 4.53978687e-05, 1.68883521e-48])def softmax(a): c = np.max(a) exp_a = np.exp(a - c) sum_exp_a = np.sum(np.exp(a - c)) y = exp_a / sum_exp_a return y# 소프트맥스 함수의 특징a = np.array([0.3, 2.9, 4.0])y = softmax(a)print(y)[0.01821127 0.24519181 0.73659691]np.sum(y)1.0소프트맥스 함수의 특징 소프트맥스 함수의 출력은 0과 1사이이다. 소프트맥스 함수의 출력의 합은 1이다.(중요)$\\therefore$ 소프트맥스 함수의 출력을 확률로 해석할 수 있다.소프트맥스 함수를 이용해서 문제를 확률적(통계적)으로 대응한다. 주의소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다. $y = \\exp(x)$가 단조 증가 함수이기 때문이다. 신경망을 이용한 분류일반적으로 가장 큰 출력을 내는 뉴런에 해당 클래스로 인식한다.소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치가 달라지지 않는다.출력층의 소프트맥스 함수를 생략해도 된다. 현업에서도 출력층의 소프트맥스 함수는 생략한다. 출력층의 뉴런 수 정하기분류에서는 분류하고 싶은 클래스 수로 설정한다.예로, 입력 이미지를 숫자 0부터 9 중 하나로 분류 : 출력층 뉴런 10개. 출력층의 뉴런은 각 숫자에 대응한다.회색 농도는 해당 뉴런의 출력 값의 크기를 의미한다. 가장 큰 값이 가장 짙은 뉴런이고, 신경망은 이 뉴런에 해당하는 클래스로 판단한다.손글씨 숫자 인식신경망의 순전파(forward propagation) : 신경망에서 추론 과정 학습 단계 : 훈련 데이터(학습 데이터)를 통해 가중치 매개변수를 학습 추론 단계 : 앞에서 학습한 매개변수를 이용해서 입력 데이터를 분류MNIST 데이터셋 MNIST: 손글씨 숫자 이미지 집합0부터 9까지 숫자 이미지로 구성. 훈련 이미지 60000장 + 시험 이미지 10000장 준비import sys, ossys.path.append(os.pardir) #부모 디렉터리 파일을 가져오도록 설정from dataset.mnist import load_mnist #dataset./mnist.py의 load_mnist 함수 임포트(x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False) # 이제 MNIST 데이터 셋을 호출하고, 인터넷 연결된 상태여야 한다.# 두번째부터는 로컬에 저장된 pickle 파일로 읽는다.# load_mnist() 함수는 (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블) 형식으로 반환한다.#각 데이터 형상 출력 print(x_train.shape)(60000, 784)print(t_train.shape)(60000,)print(x_test.shape)(10000, 784)print(t_test.shape)(10000,) load_mnist 함수인수로 normalize, flatten, one_hot_label 설정 가능하다. 세 인수 모두 bool 값이다.normalize : 입력 이미지 픽셀 값을 0.0 ~ 1.0 사이 정규화(False시 0 ~ 255)flatten : 입력 이미지를 평탄하게 1차원 배열로 설정(False시 12828)one_hot_label : 레이블을 원-핫 인코딩 형태로 저장 설정(False시 7, 2 숫자 형태 레이블 저장)#화면으로 부른다. 이미지 표시는 PIL(Python Image Library) 모듈 이용한다.import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom PIL import Imagedef img_show(img): pil_img = Image.fromarray(np.uint8(img)) pil_img.show() (x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)img = x_train[0]label = t_train[0]print(label)5print(img.shape)(784,)img = img.reshape(28, 28) # 원래 이미지 모양인 1*28*28 형태print(img.shape)(28, 28)img_show(img) 주의 사항. flatten =True로 읽으면 1차원 넘파이 배열이므로, 실제 이미지 표시할 때는 원래 형상인 28* 28로 다시 변형해야 한다.reshape() 메서드를 이용하라.PIL용 객체로 변환할 때는 image.fromarray() 메서드를 활용한다.import pickle#신경망의 추론 처리# 입력은 28 * 28 = 784개, 출력은 10개def get_data(): (x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, flatten = True, one_hot_label= False) return x_test, t_testdef init_network(): # pickle 파일인 sample_weight.pkl에 저장된 학습된 가중치 매개벼수를 읽는다. with open(\"sample_weight.pkl\", 'rb') as f: # 이 파일은 가중치와 편향 매개변수가 딕셔너리 변수로 저장된다. network = pickle.load(f) return networkdef predict(network, x): # 각 레이블의 확률 확률을 넘파이 배열로 반환한다. W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = softmax(a3) return y#정확도 평가x, t = get_data()network = init_network() #이미 학습된 가중치로 세팅이 된다.accuracy_cnt = 0for i in range(len(x)): y = predict(network, x[i]) p = np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다. if p == t[i]: accuracy_cnt += 1 print(\"정확도:\" + str(float(accuracy_cnt) / len(x)))# 맞힌 숫자의 개수를 세고, 이를 전체 이미지 숫자로 나누어 정확도를 구한다.정확도:0.9352 정규화 normalization : 0 ~ 255 범위인 픽셀 값을 0.0 ~ 1.0 범위로 변환 전처리 pre-processing : 신경망의 입력 데이터에 특정 변환을 가하는 것$\\therefore$ 입력 이미지 데이터에 대한 전처리 작업으로 정규화를 수행 데이터 백색화 whitening : 전체 데이터를 균일하게 분포시킨다.# 배치 처리 : 가중치 매개변수의 형상에 주의한다.x, _ = get_data()network = init_network()W1, W2, W3 = network['W1'], network['W2'], network['W3']x.shape(10000, 784)x[0].shape(784,)W1.shape(784, 50)W2.shape(50, 100)W3.shape(100, 10) x, _ = get_data() ?이미지 한장 입력 시, 다차원 배열의 대응하는 차원의 원소 수가 일치한다.이미지 100장 한꺼번에 입력시, 다음과 같다. 100장 분량 입력 데이터 결과가 한 번에 출력된다.예를 들어, x[0]은 0번째 이미지 + y[0]은 그 추론 결과가 저장되는 형식이다. 배치(batch) : 하나로 묶은 입력 데이터, 묶음이란 의미이다. 배치 처리는 컴퓨터 계산시 큰 이점이 있다.배치 처리로 큰 배열로 이루어진 계산을 한다.컴퓨터에서는 큰 배열을 한꺼번에 계산하는 것이 분할된 작은 배열을 여러 번 계산하는 것보다 빠르다.x, t = get_data()network = init_network()batch_size = 100 # 배치 크기accuracy_cnt = 0for i in range(0, len(x), batch_size): # 0부터 이미지 개수 -1까지 정수로 이루어진 리스트 반환하고, step 간격으로 증가한다. x_batch = x[i:i+batch_size] # 100장씩 묶는다. x[0:100], x[100:200], ... y_batch = predict(network, x_batch) p = np.argmax(y_batch, axis = 1) # argmax() 최댓값의 인덱스를 가져온다. axis = 1 주의 accuracy_cnt += np.sum(p == t[i:i+batch_size]) # p로 얻은 값이 실제 레이블과 일치하는 경우의 합이다. print(\"정확도:\" + str(float(accuracy_cnt) / len(x))) # 올바른 값 / 전체 이미지개수정확도:0.9352list( range(0, 10))[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]list( range(0, 10, 3))[0, 3, 6, 9]#np.argmax(,axis = 1)x = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])y = np.argmax(x, axis = 1) # 1번째 차원을 축으로 최댓갑의 인덱스를 찾게 한다.print(y)[1 2 1 0]y1 = np.argmax(x, axis = 0) print(y1)[3 0 1] axis = 0 이라면, 왜 저런 답이 나오는가?y1 = np.argmax(x, axis = 0) print(y1)[3 0 1]# 배치 단위로 분류한 결과를 실제 답과 비교y = np.array([1,2,1,0])t = np.array([1,2,0,0])print(y==t) # == 연산자로 넘파이 배열끼리 비교하여 bool 배열 제작# True 개수를 센다.[ True True False True]np.sum(y==t)3$\\therefore$ 데이터를 배치 처리하여 효율적이고 빠르게 처리 가능하다.정리신경망은 매끄러운 시그모이드 함수를 사용하고, 퍼셉트론은 갑자기 변하는 계단 함수를 활성화함수로 사용한다. 신경망에서는 활성화 함수로 시그모이드 함수와 ReLU 함수 같은 매끄럽게 변화하는 함수를 이용한다. 넘파이의 다차원 배열을 잘 사용하면 신경망을 효율적으로 구현할 수 있다. 기계학습 문제는 크게 회귀와 분류로 나눌 수 있다. 출력층의 활성화 함수로는 회귀에서는 주로 항등 함수를, 분류에서는 주로 소프트맥스 함수를 이용한다. 분류에서는 출력층의 뉴런 수를 분류하려는 클래스 수와 같게 설정한다. 입력 데이터를 묶은 것을 배치라 하며, 추론 처리를 이 배치 단위로 진행하면 결과를 훨씬 빠르게 얻을 수 있다." }, { "title": "밑바닥부터 시작하는 딥러닝 Chapter02", "url": "/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter02/", "categories": "ML, book_study", "tags": "Deep_learning", "date": "2021-11-14 00:00:00 +0900", "snippet": "퍼셉트론퍼셉트론 perceptron 알고리즘. 프랑크 로젠블라트가 1957년 고안한 알고리즘이고, 신경망(딥러닝)의 기원이 되는 알고리즘이다. 퍼셉트론은 딥러닝으로 나아가는 데 중요한 아이디어를 준다.퍼셉트론다수의 신호를 입력으로 하나의 신호를 출력한다. 신호 : 전류나 강물처럼 흐름 1을 신호가 흐른다. 0을 신호가 흐르지 않는다. 입력이 2개인 퍼셉트론원 : 뉴런, 노드입력 신호가 뉴런에 보내질 때는 고유한 가중치 곱해진다.뉴런에서 보내온 신호의 총합이 정해진 한계를 넘어서면 1을 출력한다. 뉴런이 활성화한다.복수의 입력 신호 각각에 고유한 가중치를 부여하고, 가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소 작용한다. 가중치가 클수록 해당 신호가 그만큼 더 중요함을 뜻한다.단순한 논리 회로AND 게이트입력이 둘이고 출력이 하나인 논리 회로. 진리표 : 입력과 출력 신호의 대응 표. 두 입력이 모두 1일 때만 1을 출력, 나머지 0을 출력.이를 퍼셉트론으로 출력해 볼 수 있다. 조건 : 진리표대로 작동하는 $w_1, w_2, \\theta$의 값을 정한다.NAND 게이트Not AND : AND 게이트 뒤집은 동작두 입력이 모두 1일 때만 0을 출력, 나머지 1을 출력 AND 게이트 매개변수의 부호를 모두 반전하기만 하면 NAND가 된다.OR 게이트입력 신호 중 하나 이상이 1이면 출력이 1이 되는 논리회로 매개 변수는 어떻게 설정할지 생각해보자?퍼셉트론의 매개변수이 매개변수 값은 인간이 설정한다. 인간은 진리표를 보면서 매개변수의 값을 생각한다.기계학습은 이 매개변수의 값을 컴퓨터가 자동으로 정하도록 한다.학습은 적절한 매개변수 값을 정하는 작업이다.퍼셉트론 구현하기#간단한 구현부터def AND(x1, x2): w1, w2, theta = 0.5, 0.5, 0.7 #함수 내에서 초기화한다. tmp = x1*w1 + x2*w2 if tmp &lt;= theta: return 0 elif tmp &gt; theta: return 1AND(0,0)0AND(1,0)0AND(0,1)0AND(1,1)1가중치와 편향 도입$\\theta 를 -b$로 치환한다.b는 편향(bias)라고 하고, $w_1, w_2$는 가중치이다.퍼셉트론은 입력신호에 가중치를 곱한 값과 편향을 합쳐서 0을 넘으면 1을 출력하고 아니면 0을 출력한다.import numpy as npx = np.array([0, 1]) # 입력 신호를 준다.w = np.array([0.5, 0.5]) # weightb = -0.7w*xarray([0. , 0.5])np.sum(w*x)0.5np.sum(w*x) + b #약 -0.2 부동소수점 수에 의한 연산 오차-0.19999999999999996#가중치와 편향 구현하기def AND(x1, x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.7 # -theta는 편향 b로 치환한다. tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1편향은 가중치와 기능이 다르다.$w_1, w_2$ 입력 신호가 결과에 주는 영향력(중요도)를 조절하는 매개변수.편향 : 뉴런이 얼마나 쉽게 활성화(1출력)을 조정하는 매개변수.편향은 균형을 깬다는 것으로 입력이 모두 0이라도 더이상 0이 나오지 않는다. 편향값이 출력되기 때문이다.# NAND, OR gatedef NAND(x1, x2): x = np.array([x1, x2]) w = np.array([-0.5, -0.5]) # AND와 가중치가 다르다! b = 0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1 #합이 0보다 작으면 0을 출력하고, 크면 1을 출력한다. 이건 동일하다.def OR(x1, x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.2 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1AND, NAND, OR은 모두 같은 구조의 퍼셉트론이고, 차이는 가중치 매개변수의 값 뿐이다.코드를 봐도 동일하다.퍼셉트론의 한계XOR 게이트!배타적 논리합이라는 논리 회로이다. 한쪽이 1일 때만 1을 출력한다.직선 하나로는 나눌 수 없다. 직선이라는 제약을 없애야 가능하다. 비선형 : 곡선의 영역 선형 : 직선의 영역“단층 퍼셉트론 single-layer perceptron으로는 XOR 게이트를 표현할 수 없다.”“단층 퍼셉트론으로는 비선형 영역을 분리할 수 없다.”다층 퍼셉트론이 출동한다면?다층 퍼셉트론(multi-layer perceptron)을 만들 수 있다. 층을 쌓아서 구성한다.기존 게이트 조합하기AND, NAND, OR 게이트를 조합한다. 진리표에 NAND 출력을 $s_1$, OR 출력을 $s_2$라고 하자. $x_1, x_2, y$를 보면 XOR이다.# XOR 게이트 구현하기def XOR(x1, x2): s1 = NAND(x1, x2) s2 = OR(x1, x2) y = AND(s1, s2) return yXOR(0,0)0XOR(0,1)1XOR(1,0)1XOR(1,1)0XOR : 다층 구조의 네트워크. XOR 퍼셉트론을 보자.왼쪽부터 0층, 1층, 2층이라 부르고, 2층 퍼셉트론이다.층이 여러 개인 퍼셉트론을 다층 퍼셉트론이라 한다.가중치를 갖는 층이 2개이므로 2층 퍼셉트론이라 부른다. 문맥에 따라 3층 퍼셉트론이라고도 한다. 0층의 두 뉴런이 입력 신호를 받아 1층의 뉴런으로 신호를 보낸다. 1층의 뉴런이 2층의 뉴런으로 신호를 보내고, 2층의 뉴런은 $y$를 출력한다. 2층 퍼셉트론의 동작은 공장에서 작업자들 사이에서 부품을 전달하는 일과 같다.단층 퍼셉트론으로는 표현하지 못한 것을 층을 하나 늘려 구현할 수 있다.NAND에서 컴퓨터까지이론상 2층 퍼셉트론이면 컴퓨터를 만들 수 있다.정리퍼셉트론은 입출력을 갖춘 알고리즘이다. 입력을 주면 정해진 규칙에 따른 값을 출력한다. 퍼셉트론은 가중치와 편향을 매개변수로 설정한다.퍼셉트론으로 AND, OR 게이트 등 논리 회로를 표현한다.다만, XOR 게이트는 단층 퍼셉트론으로는 표현할 수 없고, 2층 퍼셉트론을 이용하면 가능하다.단층 퍼셉트론은 직선형 영역만 표현할 수 있고, 다층 퍼셉트론은 비선형 영역도 표현 가능하다.다층 퍼셉트론은 이론상 컴퓨터를 표현할 수 있다." } ]
