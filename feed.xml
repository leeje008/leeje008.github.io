<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://leeje008.github.io/</id><title>Yh's blog</title><subtitle>A minimal, responsive, and powerful Jekyll theme for presenting professional writing.</subtitle> <updated>2022-09-16T15:57:54+09:00</updated> <author> <name>Koyounghun</name> <uri>https://leeje008.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://leeje008.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="ko" href="https://leeje008.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator> <rights> © 2022 Koyounghun </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>밑바닥부터 시작하는 딥러닝 Chapter08</title><link href="https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter08/" rel="alternate" type="text/html" title="밑바닥부터 시작하는 딥러닝 Chapter08" /><published>2022-01-02T00:00:00+09:00</published> <updated>2022-09-05T17:02:11+09:00</updated> <id>https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter08/</id> <content src="https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter08/" /> <author> <name>Koyounghun</name> </author> <category term="ML" /> <category term="book_study" /> <summary> Chapter 8 딥러닝 8.1 더 깊게 8.1.1 더 깊은 신경망으로 손글씨 숫자를 인식하는 심층 CNN(VGG 참고) 3X3 필터 층이 깊어질수록 채널 수 증가(16, 16, 32, 32, 64, 64) 풀링 계층으로 중간 데이터의 공간 크기가 점차 감소 활성화 함수는 ReLU 완전연결 계층 뒤에 드롭아웃 Adam optimizer He 초깃값 학습 결과 정확도 매우 높음 인식하지 못한 이미지는 대부분 인간도 판단하기 어려운 이미지 8.1.2 정확도를 더 높이려면 &amp;lt;What is the class of this image?&amp;gt;: 다양한 데이터셋을 대상으로 다양한 기법들의 정확도 순위 정리 MNIST 데이터셋 상위는 대부분 CNN ... </summary> </entry> <entry><title>밑바닥부터 시작하는 딥러닝 Chapter07</title><link href="https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter07/" rel="alternate" type="text/html" title="밑바닥부터 시작하는 딥러닝 Chapter07" /><published>2022-01-02T00:00:00+09:00</published> <updated>2022-09-05T17:02:11+09:00</updated> <id>https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter07/</id> <content src="https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter07/" /> <author> <name>Koyounghun</name> </author> <category term="ML" /> <category term="book_study" /> <summary> Chapter 7 합성곱 신경망(CNN) 7.1 전체 구조 CNN에서 등장하는 새로운 layer 합성곱 계층(convolutional layer) 풀링 계층(pooling layer) 현재까지 봤던 신경망을 완전연결(fully-connected)라고 하며 Affine 계층으로 구현했다. Affine 계층 뒤에는 활성화 함수로 ReLU 혹은 Sigmoid를 사용했다. CNN의 구조는 새로운 합성곱 계층(Conv)과 풀링 계층(pooling)이 추가된다. Conv-ReLU-(Pooling)의 흐름이다. 출력층과 가까운 층의 경우에는 Affine-ReLU, 출력층은 Affine-Softmax 조합을 그대로 사용한다. ▲ 완전연결 계층(위), CNN(아래) 7.2 합성곱 계층 패... </summary> </entry> <entry><title>Sample_post</title><link href="https://leeje008.github.io/posts/sample_post/" rel="alternate" type="text/html" title="Sample_post" /><published>2022-01-02T00:00:00+09:00</published> <updated>2022-09-04T21:19:49+09:00</updated> <id>https://leeje008.github.io/posts/sample_post/</id> <content src="https://leeje008.github.io/posts/sample_post/" /> <author> <name>Koyounghun</name> </author> <category term="ML" /> <summary> \[\left[ \begin{matrix} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{matrix} \right]\] \[\left[ \begin{array}{cc} 1 &amp;amp; 2 \\ 3 &amp;amp; 4 \\ \end{array} \right]\] $ x = {-b \pm \sqrt{b^2-4ac} \over 2a} $ \[\lim_{x\to 0}{\frac{e^x-1}{2x}} \overset{\left[\frac{0}{0}\right]}{\underset{\mathrm{H}}{=}} \lim_{x\to 0}{\frac{e^x}{2}}={\frac{1}{2}}\] </summary> </entry> <entry><title>Oversampling</title><link href="https://leeje008.github.io/posts/Oversampling/" rel="alternate" type="text/html" title="Oversampling" /><published>2021-12-04T00:00:00+09:00</published> <updated>2022-09-05T00:48:53+09:00</updated> <id>https://leeje008.github.io/posts/Oversampling/</id> <content src="https://leeje008.github.io/posts/Oversampling/" /> <author> <name>Koyounghun</name> </author> <category term="Project" /> <summary> Project-Oversampling Classification for Imabalanced Data 머신러닝 분류(classifier) 문제에서 target 변수의 범주가 불균형(imbalance) 할 경우 model의 학습 과정에서 major한 class의 데이터의 학습이 주로 이루어지므로 minor한 class의 학습이 제대로 이루어지 않는다. 이러한 imbalanced data의 경우 실제로 자주 마주할 수 있는 데이터라고 할 수 있고, 실질적으로 minor class를 잘 맞추는 것이 현실에서 주요한 머신러닝 과제라고 할 수 있다. ex) 특정 질병을 예측하는 문제의 경우 질병을 가진 사람의 데이터가 소수 class로 이루어질 가능성이 크고, 이때 정상인지를 예측하는 것보다 질병... </summary> </entry> <entry><title>밑바닥부터 시작하는 딥러닝 Chapter06</title><link href="https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter06/" rel="alternate" type="text/html" title="밑바닥부터 시작하는 딥러닝 Chapter06" /><published>2021-11-25T00:00:00+09:00</published> <updated>2022-09-05T17:02:11+09:00</updated> <id>https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter06/</id> <content src="https://leeje008.github.io/posts/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-Chapter06/" /> <author> <name>Koyounghun</name> </author> <category term="ML" /> <category term="book_study" /> <summary> Chapter 6 학습 관련 기술들 딥러닝 학습 효율, 정확도 개선 방법 가중치 매개변수 최적값 탐색하는 최적화 방법 가중치 매개변수 초깃값 하이퍼파라미터 설정 방법 오버피팅 대응책 가중치 감소 드롭아웃 배치 정규화 6.1 매개변수 갱신 신경망 학습의 목표는 매개변수의 최적값을 찾는 최적화(optimization)이다. 여태까지 사용한 최적화 방법으로 미분을 이용했는데 이 방법을 확률적 경사 하강법(SGD)라고 한다. 6.1.1 모험가 이야기 (생략) 6.1.2 확률적 경사 하강법(SGD) SGD 수식 \[W \leftarrow W - \eta \frac {\partial L} {\partial W}\] class ... </summary> </entry> </feed>
