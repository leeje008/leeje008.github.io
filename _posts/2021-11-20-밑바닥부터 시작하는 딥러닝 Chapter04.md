---
published: true
layout: post
title: "밑바닥부터 시작하는 딥러닝 Chapter04"
categories: [ML]
tags: [Deep_learning]
use_math: true
sitemap: false
---

# 밑바닥부터 시작하는 딥러닝 Chapter04

# CH04 신경망 학습

학습이란? 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 의미함

## 4.1 데이터로부터 학습한다

=> 신경망의 주요한 특징 中 하나는 데이터를 통해 가중치가 결정된다!

## 퍼셉트론 수렴 정리

내용: 선형 분리 문제는 유한번의 학습을 통해 풀 수 있다!

## 정리 
## 
$$
X^+,X^- 가\ 선형\ 분리\ 가능한\ train-set이라고\ 하자\ \\ 이\ 때\ y를\ 다음과\ 같은\ 레이블이라\ 할\ 때  y_i = \pm1(x_i \in X^\pm) \\ 전체\ train-set\ X = X^+ \cup X^-의 
\\ N개의\ 데이터셋에\ 대해\ 다음과\ 같이\ 순서를\ 입력한다고\ 하자
\\ x(1),x(2), .....x(N),x(1),.....
\\ 즉,\ 마지막\ 데이터까지\ 학습이\ 끝나면\ 처음으로\ 다시\ 돌아가는\ 것을\ 의미함
\\ 정리 예측에\ 실패한\ 입력값을\ 다음과\ 같이\ 둔다고\ 할\ 때\ x_1,x_2,x_3....x_n
\\ w_1=0 즉\ 가중치\ 초기값을\ 0이라\ 할\ 때\ 
\\ n번째\ 예측\ 실패한\ 데이터에\ 대해\ 다음과\ 같이\ 가중치를\ 업데이트\ 한다고\ 하면
\\ w_{n+1} = w_n + y_nx_n
\\ 그러면\ 다음\ 식을\ 만족하는\ n_0가\ 존재한다.
\\ w_{n0} = w_{n0+1}=w_{n0+2} .....
$$

참고 링크: https://freshrimpsushi.github.io/posts/perceptron-convergence-theorem/

## 4.1.1 데이터 주도 학습

기계학습 =>데이터에서 답을 찾고 데이터에서 패턴을 발견함 즉, 데이터가 핵심이라고 할 수 있음

신경망과 딥러닝은 기존의 기계학습에서 더 나아가 사람의 개입을 최소화해주는 기법이라고 할 수 있음

![220px-MnistExamples](/assets/images/2021-11-20-Chapter04/220px-MnistExamples.png)

다음과 같은 이미지의 분리 => 사람의 경우 손쉽게 가능 하지만 로짓을 짜는 것은 다른 문제임

기계학습에서 모아진 데이터로부터 규칙을 찾아내는 역할을 기계 즉 컴퓨터가 담당한다.

기존의 머신러닝 기법에서는 모델링은 사람이 설계하는 반면에 신경망 모형은 기계가 스스로 학습을 하게 된다.

따라서 딥러닝을 종단간 기계학습 (사람의 개입이 기존에 비해 최소화 되었다라는 의미를 내포함)이라고 한다.

## 4.1.2 train-set과 test-set

기계 학습에서는 데이터를 train-set과 test-set으로 나누게 된다.

있는 데이터를 전부 다 활용하는 것은 왜 안될까? => 일반화의 문제

즉 주어진 데이터에서만 학습을 잘하고 그 이외의 데이터에 대해서는 모델의 성능이 떨어지는 over-fitting 문제가 발생함

over-fitting은 대게 변수의 개수가 많을 때(차원의 저주라고 함) 발생하게 된다.



# 4.2 손실 함수(Loss-function)

손실 함수 => 현제 모델의 성능이 어떠한지 나타내는 지표 (정확히는 얼마나 나쁜지를 나타내는 지표)

가장 많이 사용되는 손실 함수로는 평균제곱오차(MSE)가 있음

$$
E = {1 \over 2} \sum_{k}(y_k - t_k)^2 
$$

$$
이\ 때\ y_k는\ 신경망\ 모형의\ 추정값\ t_k는\ 정답\ 레이블을\ 의미함 
$$

평균 제곱오차를 python으로 구현하는 코드는 다음과 같다


```python
import numpy as np

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```


```python
# 평균제곱 오차의 구현

def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```


```python
# 실제 사용

# 실제 정답은 2인 상황

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 예시: 2일 확률이 가장 높다고 추정함
y =  [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]

mean_squared_error(np.array(y),np.array(t)) #y와t가 리스트 형태로 저장이 되어 있으므로 넘파이 배열로 변환 후 계산

```




    0.09750000000000003




```python
# 7일 확률이 가장 높다고 추정한 경우
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

mean_squared_error(np.array(y),np.array(t))

# 위의 예시로부터 정답을 맞추면 평균제곱오차가 줄어드는 것을 확인할 수 있음
```




    0.5975



## 4.2.2 cross-entropy

신경망 모형에서 자주 사용되는 또 다른 손실 함수로 cross-entropy가 있음

$$
E =- \sum_{k}t_k\log y_k
$$

이 때 log는 일반적으로 자연상수 $$ e $$를 의미함 즉 자연로그

![다운로드 (1)](/assets/images/2021-11-20-Chapter04/다운로드 (1).png)

위 그래프에서 확인이 가능하듯히 자연로그 함수는 0에 가까울수록 y의 값이 작아짐 (손실이 작아짐)

다음은 교차 엔트로피의 python 구현 코드 

$$
이\ 때\ 아주\ 작은\ 임의의\ 수를\ 더해주어\  -\infty  가\ 되는\ 것을\ 방지해\ 줌 
$$


```python
# cross entropy 구현

def cross_entropy_error(y, t):
    delta = 1e-7 # 보정항
    return -np.sum(t * np.log(y+delta))

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

y =  [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]

cross_entropy_error(np.array(y), np.array(t))
```




    0.510825457099338




```python
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

cross_entropy_error(np.array(y), np.array(t))
```




    2.302584092994546



## 4.2.3 미니배치 학습

모든 훈련 데이터의 손실 함수 공식은 다음과 같다.

$$
E= -{1 \over N}\sum_{n}\sum_{k}t_{n,k} \log y_{n,k} 
$$

이는 단일 데이터의 손실 함수 공식을 전체 데이터의 개수 N개로 확장한 것이고, 마지막으로 데이터의 개수 N개로 나눠서

정규화된 손실함수를 구할 수 있다

하지만 일반적으로 모든 데이터의 손실함수를 구하는 것은 굉장한 computation 낭비가 발생하게 된다.

(예시에서는 60,000개의 데이터가 존재하고 일반적으로 빅데이터로 확장하게 되었을 때 계산량이 어마어마 해짐)

따라서 신경망 모형에서는 손실함수를 계산할 때 모든 훈련 데이터에 대해서 손실 함수를 구하지 않고

일부를 추출하여 (일종의 random-sampling 기법) 그 일부에 대해서만 손실 함수를 구하게 된다.

다음은 mnist 데이터에서 일부를 무작위로 골라내는 코드를 구현한 것임


```python
import sys, os
sys.path.append(os.pardir)

import numpy as np
from dataset.mnist import load_mnist

(x_train, y_train), (x_test, y_test) = load_mnist(normalize = True, one_hot_label = True) # one-hot encdoing 방식으로 class를 범주화

print(x_train.shape)

print(y_train.shape)
```

    (60000, 784)
    (60000, 10)



```python
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size) # 무작위 추출
x_batch = x_train[batch_mask]
t_batch = y_train[batch_mask]
```


```python
np.random.choice(6000, 10)

# 딥러닝에서의 배치사이즈는 통계학에서의 샘플링과 동일한 의미를 지닌다고 볼 수 있음
```




    array([ 333, 3044, 5502, 1494, 5039,  210, 1476, 2268,  245, 1344])



미니 배치 학습은 통계학에서의 sampling 기법을 적용한 것이라고 할 수 있음

통계학은 모집단의 정보를 추출하기 위해 모집단에서 표본을 추출하여 모집단의 정보를 추정하는데,

만약 적절한 표본 채집 방법과 표본의 개수를 적용한다면 굳이 모든 모집단의 표본을 가지고 있지 않아도

올바른 추정이 가능함


```python
# 배치용 cross_entropy 구현하기

def cross_entropy_error(y, t):
    if y.dim ==1:
        t = t.reshape(1, t.size) # y는 신경망에서의 추정값 t는 실제 정답 레이블을 의미하게 된다
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y+1e-7)) / batch_size
```


```python
def cross_entropy_error(y, t):
    if y.dim ==1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t]+1e-7)) / batch_size # 전체 훈련 데이터의 개수로 나눈 것이 아닌 batch_size로 나누게 된다.
```

## 4.2.5 왜 손실함수를 설정하는가?

신경망 모형에서 손실함수의 미분값을 기준으로 가중치를 갱신하는 방법을 사용함 (더 자세한 내용은 뒤에 다뤄질 듯)

따라서 정확도를 사용하게 된다면 거의 대부분의 미분 값이 0이 되기 때문에 가중치 갱신이 안된다 즉 훈련이 불가함

accuracy를 사용했을 때 미분값이 0이 된다는 것의 의미

=> 가중치를 미세하게 조정하더라도 정확도의 값은 그대로 값을 유지하는 경우가 대다수가 될 것임

하지만 손실 함수를 선택할 경우 가중치의 미세한 조정에 따라서 손실 함수의 값 역시 지속적으로 변동이 되기 때문에

그에 대한 반응이 일어나고 미분으로 계산할 시 0이 아니게 된다. 즉 가중치의 갱신이 일어난다!

![Untitled 6](/assets/images/2021-11-20-Chapter04/Untitled 6.png)

## 4.3 수치 미분

미분의 의미 => 어떤 함수가 존재하고 어떤 지점이 있을 때 해당 지점에서의 순간 변화율을 의미함 수식으로는 다음과 같다.

$$
 \lim_{h \to 0}{f(x+h)-f(x) \over h}  
$$

$$
즉\ x의\ 작은\ 변화가\ f(x)를\ 얼마나\ 변화시키는가\ 를\ 나타낸다. 
$$

다음은 미분을 python으로 구현한 코드


```python
# 나쁜 코드의 예시

def numerical_diff(f, x):
    h = 10e-50
    return ((f(x+h)) -f(x))/h # 프로그래밍에서의 반올림 오차를 발생하게 된다.

# 반올림의 문제를 어느 정도 해결

def numerical_diff(f, x):
    h = 1e-4
    return ((f(x+h)) -f(x))/h
```

결국 python으로 미분을 구현하는 것은 수치 미분을 적용하는 것과 같음

why => 애초에 실제 손실 함수가 어떠한 수식으로 이루어 졌는지 정확하게 알아내는 것이 불가능함

그러므로 근사적으로 수치미분을 사용하여 신경망 모형의 가중치를 갱신하는 것이 타당할 것임

해석적 미분 => 실제 미분값을 의미 예를 들어 
$$
{\partial \over \partial{x}} x^2 = 2x 
$$

수치 미분 =>아주 작은 차분으로 미분하는 것을 의미 즉 위의 python code로 구현한 것을 의미한다고 보면 된다. (쉽게 말해 해석적 미분의 근사치)


```python
def function_1(x):
    return 0.01*x**2 + 0.1*x
```


```python
import numpy as np
import matplotlib.pylab as plt


x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)

plt.xlabel("X")
plt.ylabel("f(X)")
plt.plot(x,y)
plt.show()
```




![다운로드](/assets/images/2021-11-20-Chapter04/다운로드.png)

```python
numerical_diff(function_1, 5) # x = 5에서의 수치미분값
```




    0.20000099999917254




```python
numerical_diff(function_1, 10) # x = 10에서의 수치미분값
```




    0.3000009999976072



## 4.3.3 편미분

일반적으로 우리가 계산하고 미분해야할 손실 함수는 여러 가중치들의 함수로 표현이 된다.

이러한 여러 변수에 대한 미분을 적용하기 위한 방법으로 편미분 방법을 적용하는데,

함수를 다음과 같이 가정할 때 편미분은 다음과 같다.


$$
f(x_0,x_1) = {x_0}^2 + {x_1}^2
$$

$$
{\partial \over \partial{x_0}} f(x_0,x_1) = 2x_0 
$$

$$
{\partial \over \partial{x_1}} f(x_0,x_1) = 2x_1
$$

이를 python으로 구현한 코드는 다음과 같다.


```python
def function_2(x):
    return x[0]**2 + x[1]**2
```


```python
def function_tmp1(x0):
    return x0*x0 + 4.0**2.0

numerical_diff(function_tmp1,3.0)
```




    6.000099999994291




```python
def function_tmp2(x1):
    return 3.0*3.0 + x1*x1

numerical_diff(function_tmp2,4.0)
```




    8.00009999998963



위의 함수의 그래프는 다음과 같이 그릴 수 있음

![img](/assets/images/2021-11-20-Chapter04/img.png)

## 4.4 기울기

기울기란? 여러 변수의 편미분 값의 vectorization 즉 여러 변수의 편미분 값을 벡터로 표현한 것이 기울기라고 할 수 있음


```python
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x) # x와 같은 형상의 배열을 생성
    
    for idx in range(x.size):
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        x[idx] = tmp_val -h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
        
    return grad
```


```python
numerical_gradient(function_2, np.array([3.0, 4.0]))
```




    array([6., 8.])




```python
numerical_gradient(function_2, np.array([0.0, 2.0]))
```




    array([0., 4.])




```python
numerical_gradient(function_2, np.array([3.0, 0.0]))
```




    array([6., 0.])



손실 함수를 convex-function으로 가정하게 된다면 해당 함수를 단면으로 자른 모습은 다음과 같다


![다운로드 (1)-16384327461131](/assets/images/2021-11-20-Chapter04/다운로드 (1)-16384327461131.png)

따라서 손실함수를 최솟값을 가지게 하려면 해당 위치에서 각각 편미분을 하여 벡터로 표현한 값에 -를 곱해주면

손실함수의 최소가 되는 지점으로 나아갈 수 있다.

이 것이 gradient-descent 방법이다.

(local_minimum 혹은 실제로 convex 형태가 아닐 가능성이 있는 문제가 있지만 이는 여기서 다루지 않고 후에 다루도록 한다.)

경사법을 수식으로 나타내면 다음과 같다.

$$
 x_0 =x_0 - \eta{\partial f \over \partial{x_0}} 
$$
$$
 x_1 =x_1 - \eta{\partial f \over \partial{x_1}} 
$$

여기서 eta는 학습률(얼마나 나아갈 지를 의미)이다.


다음은 경사하강법의 구현


```python
def gradient_descent(f, init_x, lr = 0.01, step_num = 100):
    x = init_x
    
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr*grad
        
    return x
```


```python
def function_2(x):
    return x[0]**2 + x[1]**2

init_x = np.array([-3.0, 4.0])

gradient_descent(function_2, init_x, lr = 0.01, step_num = 100)
```




    array([-0.39785867,  0.53047822])



Note: 위에서와 같이 학습률과 같은 매개변수를 hyperparameter라 하는데 이는 사용자가 지정해야 함

optimal한 hyperparameter를 찾는 방식에 대해서는 이후에 포스팅을 할 예정


```python
# 학습률이 너무 큰 예

init_x = np.array([-3.0, 4.0])

gradient_descent(function_2, init_x, lr = 10, step_num = 100)
```




    array([-2.58983747e+13, -1.29524862e+12])




```python
# 학습률이 너무 작은 예

init_x = np.array([-3.0, 4.0])

gradient_descent(function_2, init_x, lr = 1e-10, step_num = 100)
```




    array([-2.99999994,  3.99999992])



## 4.4.2 신경망에서의 기울기

신경망 학습에서의 기울기를 구하는 과정을 간단하게 수식으로 나타내면 다음과 같음 가중치를(2*3)으로 가정하고 수식 전개

$$
 W =\left[
\begin{matrix}
    w_{11} & w_{12} & w_{13} \\
    w_{21} & w_{22} & w_{23} \\
\end{matrix}
\right] 
$$


$$
 L =\left[
\begin{matrix}
    {\partial L \over \partial{w_{11}}} & {\partial L \over \partial{w_{12}}} & {\partial L \over \partial{w_{13}}} \\
    {\partial L \over \partial{w_{21}}} & {\partial L \over \partial{w_{22}}} & {\partial L \over \partial{w_{23}}} \\
\end{matrix}
\right] 
$$

이를 python으로 구현하면 다음과 같다. (초기 가중치 설정에 관련한 문제는 여기에서는 다루지 않고 뒤에서 다룰 예정)


```python
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient

class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3)
        
    def predict(self, x):
        return np.dot(x, self.W)
    
    def loss(self, x, t):
        z = self.predict(x)
        y=softmax(z)
        loss = cross_entropy_error(y, t)
        
        return loss
```


```python
net = simpleNet()
print(net.W) # 가중치 매개변수

x = np.array([0.6, 0.9])
p=net.predict(x)
print(p)

np.argmax(p) # 최댓값을 갖는 인덱스

t = np.array([0, 0, 1]) # 정답 레이블
net.loss(x, t)
```

    [[-2.07245772 -0.56657407 -1.66994278]
     [ 0.33661394  1.13914085  1.04237842]]
    [-0.94052209  0.68528232 -0.06382508]





    1.2616562177357782




```python
def f(W):
    return net.loss(x, t)
      
      
dw = numerical_gradient(f, net.W)
print(dw)
```

    [[ 0.25810928  0.03812922 -0.2962385 ]
     [ 0.38716392  0.05719382 -0.44435774]]



```python
f = lambda w: net.loss(x, t)
dw = numerical_gradient(f, net.W)
```

## 4.5 학습 알고리즘의 구현

경사하강법을 사용하여 가중치를 갱신한다. 이 때 데이터를 미니배치로 무작위로 선정하기 때문에

확률적 경사하강법(stocastic gradient descent)라 한다. 보통 SGD로 사용

mnist 데이터를 이용한 신경망 모델링


```python
from common.functions import *
from common.gradient import numerical_gradient

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size,
                weight_init_std = 0.01):
        
        # 가중치 초기화
        
        self.params ={}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
    def predict(self, x):
        W1, W2 = self.params["W1"], self.params["W2"]
        b1, b2 = self.params['b1'],  self.params['b2']
        
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
    
    # x 입력 데이터 , t: 정답 레이블
    def loss(self, x, t):
        y= self.predict(x)
        
        return cross_entropy_error(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis = 1)
        t = np.argmax(t, axis = 1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
        
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params["W1"])
        grads['b1'] = numerical_gradient(loss_W, self.params["b1"])
        grads['W2'] = numerical_gradient(loss_W, self.params["W2"])
        grads['b2'] = numerical_gradient(loss_W, self.params["b2"])
        
        return grads                
```


```python
net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)

net.params['W1'].shape
net.params['b1'].shape
net.params['W2'].shape
net.params['b2'].shape
```




    (10,)




```python
x = np.random.rand(100, 784)
y = net.predict(x)
```


```python
x = np.random.rand(100, 784)
t = np.random.rand(100, 10)

grads = net.numerical_gradient(x, t)
```


```python
print(grads['W1'].shape,
grads['b1'].shape,
grads['W2'].shape,
grads['b2'].shape)
```

    (784, 100) (100,) (100, 10) (10,)


미니 배치 학습의 구현


```python
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet


(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)

train_loss_list = []

# 하이퍼파라미터 세팅

iters_num = 1000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)

for i in range(iters_num):
    
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    grad = network.numerical_gradient(x_batch, t_batch)
    
    for key in ("W1","b1","W2","b2"):
        network.params[key] -= learning_rate*grad[key]
        
        
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
```


```python
plt.xlabel("iter_num")
plt.ylabel("Loss of model")
plt.plot(iter_num,train_loss_list)
plt.show()
```

## 4.5.3 시험 데이터로 평가하기

신경망 학습의 목표는 범용성의 확보! 따라서 훈련에 사용되지 않은 데이터를 통해 해당 모형을 평가하는 것이 중요

epoch => 하나의 단위 훈련 데이터를 미니배치를 통해 학습을 할 경우 해당 미니배치를 다 소진한다면 1epoch가 실행된 것임


```python
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet


(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)

train_loss_list = []
tarin_acc_list = []
test_acc_list = []


# 하이퍼파라미터 세팅

iters_num = 1000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

# 1에폭당 반복수

iter_per_epoch = max(train_size / batch_size, 1)

network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)

for i in range(iters_num):
    
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    grad = network.numerical_gradient(x_batch, t_batch)
    
    for key in ("W1","b1","W2","b2"):
        network.params[key] -= learning_rate*grad[key]
        
        
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    if i % iter_per_epoch ==0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        
```

## 4.6 정리

1. 기계학습에서는 train-set과 test-set으로 나누게 된다.

2. 훈련 데이터의 범용성을 확인하기 위한 방법으로 test-set을 사용하게 된다.

3. 신경망 모형의 지표로 손실함수를 사용하게 된다. 이 때 손실함수가 작은 것이 좋음

4. 해석적 미분을 구현하기 어려우므로 수치미분을 사용한다. 다음장에서 오차역전파법을 통해 이를 개선할 예정

5. 가중치 매개변수를 갱신하기 위해서 매개변수의 기울기를 이용하게 된다.
