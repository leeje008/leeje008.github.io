---
published: true
layout: post
title: "밑바닥부터 시작하는 딥러닝 Chapter08"
categories: [ML]
sitemap: false
---





# 밑바닥부터 시작하는 딥러닝 Chapter08




# Chapter 8 딥러닝

## 8.1 더 깊게

### 8.1.1 더 깊은 신경망으로

![fig 8-1](/assets/images/2022-01-02-Chapter8/fig 8-1.png)

손글씨 숫자를 인식하는 심층 CNN(VGG 참고)
- 3X3 필터
- 층이 깊어질수록 채널 수 증가(16, 16, 32, 32, 64, 64)
- 풀링 계층으로 중간 데이터의 공간 크기가 점차 감소
- 활성화 함수는 ReLU
- 완전연결 계층 뒤에 드롭아웃
- Adam optimizer
- He 초깃값

학습 결과
- 정확도 매우 높음
- 인식하지 못한 이미지는 대부분 인간도 판단하기 어려운 이미지

### 8.1.2 정확도를 더 높이려면

[<What is the class of this image?>](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html): 다양한 데이터셋을 대상으로 다양한 기법들의 정확도 순위 정리

MNIST 데이터셋
- 상위는 대부분 CNN 기반
- 그다지 깊지 않은 네트워크
- MNIST 문제는 비교적 단순하기 때문

정확도를 높일 수 있는 기술의 예
- 앙상블
- 학습률 감소
- 데이터 확장 등등

데이터 확장(data augmentation)
- 손쉽고 확실한 방법
- 훈련 이미지를 인위적으로 확장하는 방법
    - 회전
    - 이동
    - crop: 이미지 일부를 잘라내는 방법
    - flip: 좌우 반전(대칭성을 고려하지 않아도 되는 경우에만 사용)
    - 외형 변화: 밝기 조절 등
    - 스케일 변화: 확대, 축소
- 데이터가 부족할 때 효과적

### 8.1.3 깊게 하는 이유

#### 1. 매개변수의 수 감소

5X5 합성곱 연산 vs 3X3 합성곱 2회 반복

![fig 8-5](/assets/images/2022-01-02-Chapter8/fig 8-5.png)



![fig 8-6](../assets/images/2022-01-02-Chapter8/fig 8-6.png)



- 5X5 합성곱 연산은 3X3 합성곱 연산 2회로 대체 가능
- 매개 변수의 수
    - 5X5 합성곱: 25개(5X5)
    - 3X3 합성곱 2회: 18개(2X3X3)
- 매개 변수 차이는 층이 깊어질수록 더욱 커진다.

더욱 자세한 설명
- 수용 영역(receptive field): 뉴런에 변화를 일으키는 국소적인 공간 영역
- 작은 필터를 겹쳐 매개 변수 수를 줄여 넓은 수용 영역 소화 가능
- 층을 거듭할 수록 비선형인 활성화 함수를 통해 표현력 증가

#### 2. 학습의 효율성

- 층을 거듭할 수록 학습 데이터의 양이 줄어 고속으로 학습 가능
- 층을 거듭할 수록 점차 복잡한 것에 반응하는 것을 통해 알 수 있다.
- 층을 깊게 하면 학습해야 할 문제를 계층적을 분해할 수 있다. 즉, 각 층에서는 더욱 단순화된 문제를 풀게 된다.

#### 3. 정보를 계층적으로 전달할 수 있다.

초반에는 단순한 정보를 학습하고, 그 정보를 다음 층으로 넘겨 점차 고차원적인 패턴을 학습할 수 있다.

## 8.2 딥러닝의 초기 역사

ILSVRC 2012년 AlexNet을 통해 딥러닝 주목

### 8.2.1 이미지넷(ImageNet)

#### 데이터셋 설명
- 100만 장이 넘는 이미지
- 각 이미지에 레이블이 붙어 있다.
- ILSVRC에서 사용되는 데이터

<img src="https://d3i71xaburhd42.cloudfront.net/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png">

#### ILSVRC 분류(classification) 부문
- 1000개의 클래스 분류
- 채점 방식은 톱-5 오류(top-5 error): 예측 후보 클래스 5개 안에 정답이 포함되지 않을 비율
- 최우수 팀 성적 추이
![fig 8-8](/assets/images/2022-01-02-Chapter8/fig 8-8.png)
- 2012년 이후 딥러닝이 선두
- AlexNet이 오류 크게 개선
- ResNet
    - 150층이 넘는 신경망 사용
    - 일반적인 인간의 인식 능력 넘어섰다고 인정(오류율 3.5%)

### 8.2.2 VGG

- 합성곱 계층과 풀링 계층으로 구성된 기본적인 CNN  
- VGG16(16층), VGG19(19층)으로 깊은 신경망으로 심화
- 3X3 필터 2~4회 연속 이후 풀링 계층으로 크기를 절반으로 줄이는 과정 반복
- 마지막 층은 완전연결 계층
- 구성이 간단해서 응용하기 좋음

<img src="https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png">

### 8.2.3 GoogLeNet

<img src="https://miro.medium.com/max/1750/0*rbWRzjKvoGt9W3Mf.png">

- 인셉션 구조 사용
    - 크기가 다른 필터와 풀링 여러 개 적용해서 결합
    <img src="http://i.imgur.com/MqoQtOS.png" width=500>
- 1X1 합성곱 연산: 채널 쪽의 크기를 줄이는 역할, 매개변수 제거와 고속 처리에 기여

### 8.2.4 ResNet

- 층을 너무 깊게 했을 때 성능이 오히려 떨어지는 문제 해결
- 스킵 연결(skip connection)
<img src="https://i.stack.imgur.com/gSxcB.png">
- 입력 x를 연속한 두 합성곱 계층 건너뛰어 출력에 바로 연결
- 역전파 때 신호 감쇠를 막아준다
- 스킵 연결은 입력 데이터를 그대로 흐르게 해서 역전파 때 상류의 기울기를 그대로 하류로 보낸다.
- 기울기 소실 문제를 줄여준다.
- ResNet 전체 구조: VGG + skip connection
<img src="https://mblogthumb-phinf.pstatic.net/20160822_167/laonple_1471852138956ogYDD_PNG/resnet_p7_11.png?type=w2">

#### 전이 학습(transfer learning)
- 이미 학습된 가중치를 다른 신경망에 복사한 다음 그 상태로 새로운 데이터셋을 대상으로 재학습(fine tuning)을 수행해서 사용하는 것
- 데이터셋이 적을 때 유용
- ex) 이미지넷 데이터셋으로 학습한 가중치 사용

## 8.3 더 빠르게(딥러닝 고속화)

GPU 사용

### 8.3.1 풀어야 할 숙제

AlexNet forward 처리 각 층 시간 비율: GPU(좌), CPU(우)
![fig 8-14](/assets/images/2022-01-02-Chapter8/fig 8-14.png)

- 합성곱 계층에서 대부분 소요
- 단일 곱셈-누산 고속화 처리 중요

### 8.3.2 GPU를 활용한 고속화

#### GPU 컴퓨팅의 목적
- 병렬 수치 연산 고속 처리
- CPU는 연속적인 복잡한 계속 처리 용이

#### 딥러닝 CPU와 GPU 학습 비교
- GPU를 사용했을 때가 훨씬 빠르다
- 딥러닝 최적화 라이브러리 사용하면 더욱 빨라진다.
- 엔비디아의 CUDA

### 8.3.3 분산 학습

- 다수의 GPU와 기기로 계산 분산
- CNTK(computational network toolkit): 분산 학습 지원 라이브러리
- 거대한 데이터센터의 저지연, 고처리량 네트워크에서 학습 시 성능 크게 향상

어려움
- 컴퓨터 사이의 통신과 데이터 동기화

### 8.3.4 연산 정밀도와 비트 줄이기

- 메모리 용량과 버스 대역폭 등이 병목이 될 수 있다.
    - 메모리 문제: 대량의 가중치 매개변수와 중간 데이터를 메모리에 저장해야 함
    - 버스 대역폭 문제: 버스를 흐르는 데이터가 많아져 한계를 넘어서면 병목. 데이터의 비트 수 최소화 하는 것이 바람직
- 비트 수는 계산 정확도 vs 계산 비용, 메모리 사용량, 버스 대역폭 사이의 trade-off
- 신경망의 견고성: 입력 데이터가 조금 달라져도 출력 데이터는 잘 달라지지 않는 강건함을 보이는 성질
- 16비트 반정밀도만 사용해도 문제가 없다.(높은 수치 정밀도 요구 x)
    - 파스칼 아키텍쳐에서 지원
    - 파이썬: 64비트, 넘파이: 16비트도 지원
    - Binarized Neural Networks: 가중치와 중간 데이터를 1비트로 표현하는 방법 연구 

컴퓨터의 실수 표현 방식
- 32비트 단정밀도
- 64비트 배정밀도
- 16비트 반정밀도

## 8.4 딥러닝의 활용

### 8.4.1 사물 검출

<img src="https://mblogthumb-phinf.pstatic.net/MjAxODA3MDFfMzAw/MDAxNTMwNDU2OTEyNzEz.N3nj4DtudgW9SQoQGXv7hTvUQqY8CQgKAejziILXpe8g.HXnr16aCPHZbMr8YXBSLYfIy8Wmvp14bGxlVt0ShyHcg.JPEG.linuxand/%EC%82%AC%EB%AC%BC%EA%B2%80%EC%B6%9C.jpg?type=w800">

- 이미지 속에 담긴 사물의 위치와 종류를 알아내는 기술
- 하나의 이미지에 여러 사물 존재할 수 있다.
- R-CNN(regions with convolutional neural networks)
<img src="https://tensorflowkorea.files.wordpress.com/2017/06/0sdj6skdrqyzpo6oh.png?w=625">
- 후보 영역 추출과 CNN 특징 계산이 가장 큰 특징
    - 후보 영역 추출: Selective Search 기법 사용
    - 후보 영역 추출까지 CNN으로 처리하는 Faster R-CNN 등장

### 8.4.2 분할(segmentation)

- 픽셀 수준으로 분류
- 픽셀 단위로 객체마다 채색된 지도 데이터를 사용해 학습
- 추론: 입력 이미지의 모든 픽셀 분류
<img src="http://vladlen.info/wp-content/uploads/FSO-1.jpg">
- 모든 픽셀 대상으로 추론하면 픽셀 수만큼 forward 처리해야 되서 비효율적

#### FCN(fully convolutional network)
<img src="https://mblogthumb-phinf.pstatic.net/MjAxNzAzMTRfNzkg/MDAxNDg5NDkwNjAxMjY3.UEJzb5nzWcN94UErndLiJp7pf6ljxA6Neh5-AcOMk40g.inO_1esH3LRHew6JNPDd8-NQp-5qu7VNMxpxmHFfQ1wg.PNG.laonple/%EC%9D%B4%EB%AF%B8%EC%A7%80_16.png?type=w2">

- 한번의 forward 처리로 모든 픽셀의 클래스를 분류해주는 기법
- 완전연결 계층을 같은 기능을 하는 합성곱 계층으로 바꾼다.
- 공간 볼륨을 유지한 채 마지막 출력 처리 가능
- FCN의 마지막 층: 공간의 크기 확대
    - 이중 선형 보간(bilinear interpolation)에 의한 선형 확대
    - 역합성곱(deconvolution) 연산으로 구현

### 8.4.3 사진 캡션 생성

![Screen-Shot-2014-11-17-at-2.11.11-PM-550x355](/assets/images/2022-01-02-Chapter8/Screen-Shot-2014-11-17-at-2.11.11-PM-550x355.png)

<img src="http://www.techfrontier.kr/wp-content/uploads/2014/11/Screen-Shot-2014-11-17-at-2.11.11-PM-550x355.png">

- 사진을 주면 그 사진을 설명하는 글을 자동으로 생성
- NIC(neural image caption) 모델
    - 심층 CNN + 순환 신경망(recurrent neural network, RNN)
    - CNN으로 사진 특징 추출
    - 특징을 RNN에 넘겨 특징을 추깃값으로 텍스트를 순환적으로 생성
- 멀티모달 처리(multimodal processing): 여러 종류의 정보르 조합, 처리
<img src="https://t1.daumcdn.net/cfile/tistory/99DA9B405B596D2F0B">

RNN 간단 설명
- 순환적 네트워크 구조
- 과거의 정보를 기억하는 특징
- 연속성 있는 데이터 다룰 때 주로 사용

## 8.5 딥러닝의 미래

### 8.5.1 이미지 스타일(화풍) 변환

<img src="https://bloglunit.files.wordpress.com/2017/04/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2017-05-16-e1848be185a9e18492e185ae-1-56-54.png?w=520&h=392">

- 두 이미지를 입력해서 새로운 그림 생성
    - 콘텐츠 이미지
    - 스타일 이미지
- A Neural Algorithm of Artistic Style 논문

학습 방식
- 네트워크의 중간 데이터가 콘텐츠 이미지의 중간 데이터와 비슷해지도록 학습
- 스타일 행렬의 오차를 줄이도록 학습해서 스타일 이미지의 화풍을 흡수하도록 한다.

### 8.5.2 이미지 생성

- 대량의 이미지를 학습한 후 입력 이미지 없이 새로운 이미지 생성
- DCGAN(deep convolutional generative adversarial network)
- 생성자(generator): 진짜와 똑같은 이미지를 생성
- 식별자(discriminator): 생성자가 생성한 이미지를 판별
- 생성자와 식별자를 겨루도록 학습해서 정교한 가짜 이미지를 생성해내도록 한다.

지도학습(supervised learning)과 자율학습(unsupervised learning)
- 지도학습: 입력 데이터와 정답 레이블을 짝지은 데이터셋을 이용해서 학습
- 자율학습: 지도용 데이터 없이 스스로 학습
    - Deep Belief Network, Deep Boltzmann Machine

### 8.5.3 자율 주행

- 안전한 주행 영역을 인식하는 것이 중요
- SegNet: 주변 환경 인식하는 CNN 기반 신경망
    - 픽셀 수준에서 분할

### 8.5.4 Deep Q-Network(강화 학습)

강화학습
- 에이전트: 주어진 환경에서 행동을 선택하는 주체
- 환경: 에이전트의 행동에 영향을 주는 조건
- 보상: 에이전트가 환경을 변화시키는 행동을 했을 때 에이전트에게 보상이 주어지고, 더 나은 보상을 받는 쪽으로 에이전트를 학습시킨다.

Deep Q-Network(DQN)
- 딥러닝을 사용한 강화학습
- Q학습에 기초
    - 최적 행동 가치 함수로 최적 행동 정한다.
- 최적 가치 함수를 CNN으로 비슷하게 흉내내어 사용
- 입력 데이터로 게임 영상만 주면 된다.
    - 기존의 학습에선 게임의 상태를 미리 출력해야 했다.
- 구성을 변경하지 않고 적용 가능
- 게임 영상 프레임을 입력해서 게임을 제어하는 움직임에 대햐여 각 동작의 가치를 출력
- 알파고, 팩맨, 아타리 등 많은 게임에서 사람보다 뛰어난 성적
<img src="https://curt-park.github.io/images/dqn/architecture.png">
