---
published: true
layout: post
title: "케라스 창시자에게 배우는 딥러닝 Chapter02"
categories: [ML]
math: true
sitemap: false
---

# 케라스 창시자에게 배우는 딥러닝 Chapter02

# ch02 신경망의 수학적 구성요소


```python
import warnings
warnings.filterwarnings(action='ignore')
```

## 2-1 신경망의 기본

용어 정의

class: 머신 러닝에서 분류문제의 범주를 의미함

sample: 머신 러닝에서 사용하게 될 데이터 포인터를 의미함

label: 특정 샘플의 클래스를 레이블이라고 함

train set: 머신 러닝에서 학습해야하는 대상

test set: 특정 머신 러닝 모델의 성능을 평가하는 대상 


```python
# mnist data 로드
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
```

    Using TensorFlow backend.



```python
train_images.shape # train set 구조
```




    (60000, 28, 28)




```python
len(train_labels) # 총 60000개의 sample이 담겨 있음
```




    60000




```python
train_labels
```




    array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)




```python
test_images.shape # test set의 구조
```




    (10000, 28, 28)




```python
len(test_labels) # 총 10000개의 sample이 담겨 있음
```




    10000




```python
test_labels
```




    array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)




```python
# 신경망의 구조
from tensorflow.keras import models
from tensorflow.keras import layers 

network = models.Sequential()
network.add(layers.Dense(512, activation = 'relu', input_shape = (28 * 28, ))) # 첫번째 신경망 구조 전체 512개로 구성
network.add(layers.Dense(10, activation = 'softmax')) # 두번째 신경망 구조 최종신경망이므로 softmax를 활성화함수로 분류문제 적용
```

## activation function

### 활성화 함수: 신경망 구조에서 input과 적절한 가중치로 출력된 값을 적절하게 처리하는 함수

<img src="https://cs231n.github.io/assets/nn1/neuron.png">


<img src="https://cs231n.github.io/assets/nn1/neuron_model.jpeg">

# 활성화 함수의 종류

<img src="https://miro.medium.com/max/1192/1*4ZEDRpFuCIpUjNgjDdT2Lg.png">



1. 손실함수: 훈련 데이터에서 신경망의 성능을 측정하는 방법 더 나은 방향으로 학습될 수 있도록 도와준다.

2. 옵티마이저: 입력된 데이터와 손실함수를 기반으로 네트워크를 업데이트하는 메커니즘

3. 훈련 및 테스트 과정을 모니터링할 지표: 해당 신경망 모델을 평가하는 지표


```python
network.compile(optimizer = 'rmsprop',
               loss = 'binary_crossentropy',
               metrics = ['accuracy']) # 정확도를 지표로 손실함수 및 최적화방법 정의
```


```python
# 신경망 모델에 맞게 데이터 변환 min-max scaling을 적용

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255 

test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255 # min-max scaling 시도
```


```python
# target 데이터의 범주화
from tensorflow.keras.utils import to_categorical

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
```


```python
# 모델 훈련
network.fit(train_images, train_labels, epochs = 5, batch_size = 128)
```

    Train on 60000 samples
    Epoch 1/5
    60000/60000 [==============================] - 6s 99us/sample - loss: 0.0055 - accuracy: 0.9983
    Epoch 2/5
    60000/60000 [==============================] - 7s 110us/sample - loss: 0.0042 - accuracy: 0.9986
    Epoch 3/5
    60000/60000 [==============================] - 6s 106us/sample - loss: 0.0034 - accuracy: 0.9990
    Epoch 4/5
    60000/60000 [==============================] - 6s 97us/sample - loss: 0.0026 - accuracy: 0.9992
    Epoch 5/5
    60000/60000 [==============================] - 6s 97us/sample - loss: 0.0021 - accuracy: 0.9994





    <tensorflow.python.keras.callbacks.History at 0x1b344ee3cc0>




```python
test_labels.shape
```




    (10000, 10)




```python
test_loss, test_acc = network.evaluate(test_images, test_labels)
print('test_acc:', test_acc)
```

    10000/1 [==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 111us/sample - loss: 0.0059 - accuracy: 0.9963
    test_acc: 0.9963494


## 신경망 모델에서의 데이터 표현


```python
# 스칼라
import numpy as np
x = np.array(12)
x
```




    array(12)




```python
x.ndim
```




    0




```python
# 1D텐서 => 일상적으로 사용하는 벡터와 같음
x = np.array([12, 3, 6, 14, 7])
x
```




    array([12,  3,  6, 14,  7])




```python
x.ndim
```




    1




```python
# 2D 텐서 => 흔히 말하는 행렬과 같은 구조를 취함
x = np.array([[5, 78, 2, 34, 0],
             [6, 79, 3, 35, 1],
             [7, 80, 4, 36, 2]])
x.ndim
```




    2




```python
x = np.array([[[5, 78, 2, 34, 0],
              [6, 79, 3, 35, 1],
              [7, 80, 4, 36, 2]],
             [[5, 78, 2, 34, 0],
              [6, 79, 2, 35, 1],
              [7, 80, 4, 36, 2]],
              [[5, 78, 2, 34, 0],
               [6, 79, 3, 35, 1],
               [7, 80, 4, 36, 2]]])
x.ndim # 기존의 텐서들에서 새로운 배열로 합치면 고차원의 텐서로 변환이 가능함

## 보통의 경우 4D텐서까지 다루고 동영상의 경우 5D텐서까지 가기도 한다.
```




    3



## 텐서의 핵심속성

1. 축의 개수 (랭크): 행렬을 2개의 축, 벡터는 1개의 축을 가지고 있음
2. 크기: 각 축에 따라 얼마나 많은 차원이 있는지를 나타냄
3. 데이터 타입: dtype에 저장이 된다 보통 float32, 64, uint8이 사용된다


```python
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
```


```python
print(train_images.ndim)
```

    3



```python
print(train_images.dtype)
```

    uint8



```python
# 이미지 출력해보기

digit = train_images[4]

import matplotlib.pyplot as plt
plt.imshow(digit, cmap = plt.cm.binary)
plt.show()
```



![다운로드](/assets/images/2022-01-02-keras study for deeplearning ch02/다운로드.png)




###  슬라이싱 => 데이터에서 특정 원소를 선택하는 것


```python
my_slice = train_images[10: 100]
print(my_slice.shape)
```

    (90, 28, 28)



```python
my_slice = train_images[10:100, :, :]
my_slice.shape # 위의 결과와 동일한 코드
```




    (90, 28, 28)




```python
my_slice = train_images[10:100, 0:28, 0:28]
my_slice.shape # 역시 동일한 결과값을 내줌
```




    (90, 28, 28)




```python
my_slice = train_images[:, 14:, 14:]
my_slice.shape
```




    (60000, 14, 14)




```python
# 음수 인덱스도 사용이 가능함
my_slice = train_images[:, 7:-7, 7:-7]
my_slice.shape
```




    (60000, 14, 14)



### 배치 데이터 => 일반적으로 딥러닝에서 사용되는 데이터의 첫번째 축이 샘플 축

ex) (60000, 28, 28) => 60000에 해당하는 0번째 축이 샘플축에 해당함

### 딥러닝에서 일반적으로 모든 데이터를 한번에 처리하지 않고  샘플림을 통해 데이터의 입력을 받음

### 텐서의 실제 사례
1. 벡터 데이터 => sample, features로 이루어진 2D tensor
2. 시계열 혹은 시퀀스형 데이터 => sample, timestamp, features로 이루어진 3D tensor
3. 이미지 => sample, height, width, channels 혹은 chnnels, height, width로 이루어진 4D tensor
4. sample, frames, height, width, channels 혹은 chnnels, height, width로 이루어진 5D tensor

### 텐서의 연산 


```python
def naive_relu(x):
    assert len(x.shape) == 2 # 2차원 배열의 형태로 저장
    
    x = x.copy() # 텐서의 변화가 없도록 복사
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] = max(x[i,j], 0)
    return x
```


```python
def naive_add(x, y):
    assert len(x.shape) == 2
    assert x.shape == y.shape
    
    x = x.copy(0)
    for i in range(x.shape[0]):
        for j in range(y.shape[1]):
            x[i, j] += y[i, j]
    return x # 텐서의 덧셈 연산
```


```python
import numpy as np
z = x + y
z = np.maximum(z, 0.)
```


    ---------------------------------------------------------------------------
    
    NameError                                 Traceback (most recent call last)
    
    <ipython-input-84-3ccc27131848> in <module>
          1 import numpy as np
    ----> 2 z = x + y
          3 z = np.maximum(z, 0.)


    NameError: name 'y' is not defined


## 브로드캐스팅
=> 크기가 다른 두 개의 텐서의 연산을 가능하게 만들어주는 방법
1. 큰 텐서의 ndim에 맞도록 작은 텐서의 축이 추가
2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복


```python
def naive_add_matrix_and_vector(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) ==1
    assert x.shape[1] == y.shape[0]
    
    x = x.copy()
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[j]
    return x
```


```python
import numpy as np

x = np.random.random((64, 3, 32, 10))
y = np.random.random((32, 10))

z = np.maximum(x, y)
```


```python
# 텐서의 점곱은 보통 dot을 통해 코딩 구현

import numpy as np

z = np.dot(x, y)

z = x * y

```


    ---------------------------------------------------------------------------
    
    ValueError                                Traceback (most recent call last)
    
    <ipython-input-87-a48277f4855c> in <module>
          3 import numpy as np
          4 
    ----> 5 z = np.dot(x, y)
          6 
          7 z = x * y


    ValueError: shapes (64,3,32,10) and (32,10) not aligned: 10 (dim 3) != 32 (dim 0)



```python
def naive_vector_dot(x, y):
    assert len(x.shape) == 1
    assert len(y.shape) ==1
    assert x.shape[0] == y.shape[0]
    
    z = 0.
    for i in range(x.shape[0]):
        z += x[i] * y[i]
    return z
```


```python
# 행렬과 벡터사이의 점곱
import numpy as np

def naive_matri_vector_dot(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 1
    assert x.shape[1] == y.shape[0]
    
    z = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            z[i] += x[i, j] * y[j]
    return z
```


```python
def naive_matrix_dot(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 2
    assert x.shape[1] == y.shape[0]
    
    z = np.zeros(x.shape[0, y.shape[1]])
    for i in range(x.shape[0]):
        for j in range(y.shape[1]):
            row_x = x[i, :]
            column_y = y[:, j]
            z[i, j] = naive_matrix_dot(row_x, column_y)
    return z
```

## 텐서 크기의 변환

주로 신경망 모델에 들어갈 숫자 데이터를 전처리할 경우에 사용


```python
train_images = train_images.reshape((60000, 28 * 28))
```


```python
x = np.array([[0., 1.],
            [2., 3.], 
            [4., 5.]])
print(x.shape)
```

    (3, 2)



```python
x = x.reshape((6, 1))
x
```




    array([[0.],
           [1.],
           [2.],
           [3.],
           [4.],
           [5.]])




```python
x = x.reshape((2, 3))
x
```




    array([[0., 1., 2.],
           [3., 4., 5.]])



### => 텐서의 크기 변환시 자주 사용되는 변환은 전치(transposition) : 행과 열의 교환


```python
x = np.zeros((300, 20))
x = np.transpose(x)
print(x.shape)
```

    (20, 300)


## 텐서 연산의 기하학적 해석

모든 텐서의 연산은 기본적으로 기하학적으로 해석이 가능함

모든 신경망 모델은 이러한 복잡한 기하학적 표현을 조금씩 분해하는 방식을 택함 

# 2.4 신경망의 엔진 그래디언트 기반 최적화

## W,b는 각 층의 속성 초기 단계에서는 이러한 파라미터들을 임의로 설정

## 임의로 설정된 파라미터들로부터 계속 train(가중치 조정단계)를 거쳐 유용한 W,b를 찾아내는 것이 핵심


```python
# 옵티마이저 모멘텀 구현

past_velocity = 0.
momentmum = 0.1
while loss > 0.01:
    w, loss, gradient, = get_current_parameters()
    velocity = momentmum * past_velocity - learning_rate * graidient
    w = w + momentmum * velocity - learning_rate * gradient
    past_velocity = velocity
    update_parameter(w)
```


    ---------------------------------------------------------------------------
    
    NameError                                 Traceback (most recent call last)
    
    <ipython-input-102-a784b7bd744c> in <module>
          3 past_velocity = 0.
          4 momentmum = 0.1
    ----> 5 while loss > 0.01:
          6     w, loss, gradient, = get_current_parameters()
          7     velocity = momentmum * past_velocity - learning_rate * graidient


    NameError: name 'loss' is not defined



```python
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape(60000, 28 * 28)
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape(10000, 28 * 28)
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)


network = models.Sequential()
network.add(layers.Dense(512, activation = 'relu', input_shape = (28 * 28,)))
network.add(layers.Dense(10, activation = 'softmax'))

network.compile(optimizer = 'rmsprop',
               loss = 'categorical_crossentropy',
               metrics = ['accuracy'])


```


```python
network.fit(train_images, train_labels, epochs = 5, batch_size = 128)
```

    Train on 60000 samples
    Epoch 1/5
    60000/60000 [==============================] - 5s 80us/sample - loss: 0.2553 - accuracy: 0.9257
    Epoch 2/5
    60000/60000 [==============================] - 4s 70us/sample - loss: 0.1036 - accuracy: 0.9692
    Epoch 3/5
    60000/60000 [==============================] - 4s 75us/sample - loss: 0.0673 - accuracy: 0.9798
    Epoch 4/5
    60000/60000 [==============================] - 5s 76us/sample - loss: 0.0489 - accuracy: 0.9852
    Epoch 5/5
    60000/60000 [==============================] - 5s 76us/sample - loss: 0.0369 - accuracy: 0.9890





    <tensorflow.python.keras.callbacks.History at 0x25826af1dd8>

